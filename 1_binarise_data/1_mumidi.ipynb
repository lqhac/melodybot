{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae58a9a-b016-4a2a-9c9b-80eba74bc4d3",
   "metadata": {},
   "source": [
    "### MuMIDI Whole Song Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2e707765-742a-4328-a890-85f5bacd0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['PYTHONPATH'] = '/home/qihao/CS6207'\n",
    "sys.path.append('/home/qihao/CS6207')\n",
    "import miditoolkit\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle, glob\n",
    "from tqdm import tqdm\n",
    "from utils.indexed_datasets import IndexedDatasetBuilder\n",
    "import multiprocessing as mp\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ddae1774-fc4e-4dc4-896f-8f9beb5bc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = \"/home/qihao/CS6207/MelodyGLM/MDP/data/processed/wikifonia/7_dedup/wikifonia_1001_seg0_1_Seg1.mid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9bf5daf0-4f6b-4f1e-a5cb-86deaac475e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_melody (midi_pth: str):\n",
    "    group_by_seg = {}\n",
    "    basic_melody = {}\n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    assert len(midi.instruments) == 1  ## monophonic\n",
    "    seg_len = midi.ticks_per_beat * 2\n",
    "    \n",
    "    for note in midi.instruments[0].notes:\n",
    "        seg_id = note.start // seg_len\n",
    "        if seg_id not in group_by_seg.keys():\n",
    "            group_by_seg[seg_id] = []\n",
    "        group_by_seg[seg_id].append(note)\n",
    "    \n",
    "    for seg_id, seg_notes in group_by_seg.items():\n",
    "        freq = {}\n",
    "        for note in seg_notes:\n",
    "            if note.pitch not in freq.keys():\n",
    "                freq[note.pitch] = 0\n",
    "            freq[note.pitch] += 1\n",
    "    \n",
    "    return basic_melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6531ba3e-e10c-46b7-b3f1-9e9240132fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress (start, dur, reso=480):\n",
    "    if dur in [reso, reso//2, reso//4, reso//8, reso//16, reso*2, reso*4]:\n",
    "        ## categorise the note duration\n",
    "        unit_len = 4 * dur\n",
    "    else:\n",
    "        unit_len = 4 * reso\n",
    "    beat_pos = start - (start//unit_len) * unit_len\n",
    "    beat_num = beat_pos // (unit_len//4)\n",
    "    # beat_num = start % unit_len\n",
    "    # print(f\"dur:{dur}, pos:{beat_pos}, beat:{beat_num}, unit:{unit_len}\")\n",
    "    if beat_num == 0:\n",
    "        return \"<strong>\"\n",
    "    elif beat_num == 2:\n",
    "        return \"<substrong>\"\n",
    "    else:\n",
    "        return \"<weak>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7cb5e68e-6eee-43b4-a31d-65dfb77a1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prosody_avg_ver (midi_pth: str):\n",
    "    ## use mean length to decide the overall length\n",
    "    prosody = []\n",
    "    \n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    ## group by bar:\n",
    "    bar = {}\n",
    "    ## calculate average note length\n",
    "    note_durs = []\n",
    "    strength, length = [], []\n",
    "    reso = midi.ticks_per_beat\n",
    "    for inst in midi.instruments:\n",
    "        for i, note in enumerate(inst.notes):\n",
    "            bar_num = np.floor(note.start/(8*reso))\n",
    "            if bar_num not in bar.keys():\n",
    "                bar[bar_num] = []\n",
    "            bar[bar_num].append(note)\n",
    "            dur = note.end - note.start\n",
    "            note_durs.append(dur)\n",
    "    ## calculate global average\n",
    "    avg_dur = np.mean(note_durs)\n",
    "    ## calculate bar level average\n",
    "    bar_avg = {}\n",
    "    for bar_num, bar_notes in bar.items():\n",
    "        bar_avg[bar_num] = np.mean([note.end-note.start for note in bar_notes])\n",
    "    for bar_num, bar_notes in bar.items():\n",
    "        for note in bar_notes:\n",
    "            strength = stress(start=note.start, dur=note.end-note.start, reso=reso)\n",
    "            length = \"<long>\" if note.end-note.start>bar_avg[bar_num] else \"<short>\"\n",
    "            # prosody.append(f\"<{strength}, {length}>\")\n",
    "            prosody.append((strength, length))\n",
    "    \n",
    "    return prosody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1bbdf90f-57ab-43f6-b825-c1bb20d35190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prosody (midi_pth: str):\n",
    "    ## use absolute value\n",
    "    prosody = []\n",
    "    \n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    ## group by bar:\n",
    "    bar = {}\n",
    "    ## calculate average note length\n",
    "    note_durs = []\n",
    "    strength, length = [], []\n",
    "    reso = midi.ticks_per_beat\n",
    "    for inst in midi.instruments:\n",
    "        for i, note in enumerate(inst.notes):\n",
    "            strength = stress(start=note.start, dur=note.end-note.start, reso=reso)\n",
    "            length = \"<long>\" if note.end-note.start>reso else \"<short>\"\n",
    "            prosody.append((strength, length))\n",
    "    \n",
    "    return prosody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d1f710cd-9f29-4666-95a3-3009c3965779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n"
     ]
    }
   ],
   "source": [
    "print(len(prosody(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "41ccd925-baea-4c88-9eb3-fc7ba086e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrasing (midi_pth: str):\n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    assert len(midi.instruments) == 1  ## monophonic\n",
    "    reso = midi.ticks_per_beat\n",
    "    notes = midi.instruments[0].notes.copy()\n",
    "    \n",
    "    long = []\n",
    "    pause = []\n",
    "    note_info = []\n",
    "    \n",
    "    for idx, note in enumerate(notes):\n",
    "        note_bar = int(np.floor(note.start / (4 * reso))) ## a bar == 4 beat == 4 * 480 ticks\n",
    "        note_pos = (note.start - (note_bar * 4 * reso)) ## relative position in the current bar\n",
    "        note_pitch = note.pitch\n",
    "        note_dur = note.end - note.start\n",
    "        note_info.append((note_bar, note_pos, note_pitch, note_dur))\n",
    "        if note_dur > reso:\n",
    "            long.append(idx)\n",
    "        if (idx > 0) and (notes[idx].start-notes[idx-1].end >= reso//2):\n",
    "            pause.append(idx-1)\n",
    "    \n",
    "    union = list(set(long + pause))\n",
    "    if 0 in union:\n",
    "        union.remove(0)\n",
    "    if len(notes)-1 in union:\n",
    "        union.remove(len(notes)-1)\n",
    "    union.sort()\n",
    "    \n",
    "    def dur(note: miditoolkit.Note):\n",
    "        return abs(note.end-note.start)\n",
    "    \n",
    "    i = 1\n",
    "    while i<len(union):\n",
    "        if abs(union[i-1]-union[i]) == 1:\n",
    "            if abs(dur(notes[union[i-1]])-dur(notes[union[i]])) > 240:\n",
    "                union.remove(union[i])\n",
    "            else:\n",
    "                union.remove(union[i-1])\n",
    "        i = i + 1\n",
    "    \n",
    "    ### annotate\n",
    "    midi.markers=[]\n",
    "    for k, b in enumerate(union):\n",
    "        midi.markers.append(miditoolkit.Marker(time=notes[b].end, text=f\"Phrase_{k}\"))\n",
    "    \n",
    "    # midi.dump(os.path.join('./', os.path.basename(midi_pth)[:-4]+'_phrased.mid'))\n",
    "    \n",
    "    is_boundary = []\n",
    "    for i in range(len(notes)):\n",
    "        if i in union:\n",
    "            is_boundary.append(\"<true>\")\n",
    "        else:\n",
    "            is_boundary.append(\"<false>\")\n",
    "    \n",
    "    assert len(note_info) == len(is_boundary)\n",
    "    return is_boundary, note_info, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e4b1c97e-44d8-4bfc-a6a4-3651fbf2ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes (midi_pth: str):\n",
    "    midi = miditoolkit.MidiFile(midi_pth)\n",
    "    assert len(midi.instruments) == 1  ## monophonic\n",
    "    reso = midi.ticks_per_beat\n",
    "    notes = midi.instruments[0].notes.copy()\n",
    "    \n",
    "    note_info = []\n",
    "    \n",
    "    for note in notes:\n",
    "        note_info.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "769712d4-4920-4eab-8717-34d8b9891401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 278\n"
     ]
    }
   ],
   "source": [
    "bound, note_info, _ = phrasing(test_sample)\n",
    "print(len(bound), len(note_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "94e8b17b-fd9a-42ec-80d0-e27d525d9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise (midi_pth, event2word_dict):\n",
    "    prsd = prosody(midi_pth)\n",
    "    bound, notes, _ = phrasing(midi_pth)\n",
    "    assert len(prsd) == len(bound)\n",
    "    src_words, tgt_words = [], []\n",
    "    global_bar_id = 0\n",
    "    last_item_bar = 0\n",
    "    last_item_pos = -1\n",
    "    \n",
    "    ## bos\n",
    "    src_words.append({\n",
    "        'strength':event2word_dict['Prosody']['<s>'],\n",
    "        'length':0,\n",
    "        'phrase':0,\n",
    "    })\n",
    "    \n",
    "    tgt_words.append({\n",
    "        'bar': 0,\n",
    "        'pos': 0,\n",
    "        'token':event2word_dict['Token'][f\"<s>\"],\n",
    "        'dur': 0,\n",
    "        'phrase': 0,\n",
    "    })\n",
    "\n",
    "    src_sent_id = 0\n",
    "    for idx in range(len(prsd)):\n",
    "        strength, length = prsd[idx][0], prsd[idx][1]\n",
    "        prosody_word = f\"{strength[:-1]}, {length[1:]}\"\n",
    "        if idx == 0:\n",
    "            src_words.append({\n",
    "                'strength':event2word_dict['Prosody'][f\"<sent>\"],\n",
    "                'length':0,\n",
    "                'phrase':0,\n",
    "            })\n",
    "            src_sent_id += 1\n",
    "        ## vanilla version\n",
    "        ## step1: append strength\n",
    "        src_words.append({\n",
    "            'strength':event2word_dict['Prosody'][strength],\n",
    "            'length':0,\n",
    "            'phrase':0,\n",
    "        })\n",
    "        ## step2: append length\n",
    "        src_words.append({\n",
    "            'strength':event2word_dict['Prosody'][length],\n",
    "            'length':0,\n",
    "            'phrase':0,\n",
    "        })\n",
    "        ## step3: append sentence splitter\n",
    "        if bound[idx] == '<true>':\n",
    "            src_words.append({\n",
    "                'strength':event2word_dict['Prosody'][f'<sent>'],\n",
    "                'length':0,\n",
    "                'phrase':0,\n",
    "            })\n",
    "            src_sent_id += 1\n",
    "    \n",
    "    src_words.append({\n",
    "        'strength':event2word_dict['Prosody']['</s>'],\n",
    "        'length':0,\n",
    "        'phrase':0,\n",
    "    })\n",
    "    '''\n",
    "    ## compound word version\n",
    "    src_words.append({\n",
    "        'strength':event2word_dict['Prosody'][prosody_word],\n",
    "        'length':0,\n",
    "        'phrase':0,\n",
    "    })\n",
    "    '''\n",
    "    \n",
    "    tgt_sent_id = 0\n",
    "    for idx in range(len(notes)):\n",
    "        if idx == 0:\n",
    "            tgt_words.append({\n",
    "                'bar':0,\n",
    "                'pos':0,\n",
    "                'token':event2word_dict['Token'][f\"<sent>\"],\n",
    "                'dur':0,\n",
    "                'phrase':0,\n",
    "            })\n",
    "            tgt_sent_id += 1\n",
    "        note_bar = notes[idx][0]\n",
    "        while global_bar_id <= note_bar:\n",
    "            global_bar_id += 1\n",
    "            tgt_words.append({\n",
    "                'bar':0,\n",
    "                'pos':0,\n",
    "                'token':event2word_dict['Token'][f\"Bar\"],\n",
    "                'dur':0,\n",
    "                'phrase':0,\n",
    "            })\n",
    "            last_item_pos = 0\n",
    "        \n",
    "        note_pos = notes[idx][1]\n",
    "        if global_bar_id == last_item_bar and last_item_pos == note_pos:\n",
    "            continue\n",
    "        tgt_words.append({\n",
    "            'bar':0,\n",
    "            'pos':0,\n",
    "            'token':event2word_dict['Token'][f\"Pos_{note_pos}\"],\n",
    "            'dur':0,\n",
    "            'phrase':0,\n",
    "        })\n",
    "        last_item_pos = note_pos\n",
    "        last_item_bar = global_bar_id\n",
    "        \n",
    "        \n",
    "        tgt_words.append({\n",
    "            'bar':0,\n",
    "            'pos':0,\n",
    "            'token':event2word_dict['Token'][f\"Pitch_{notes[idx][2]}\"],\n",
    "            'dur':0,\n",
    "            'phrase':0,\n",
    "        })\n",
    "        tgt_words.append({\n",
    "            'bar':0,\n",
    "            'pos':0,\n",
    "            'token':event2word_dict['Token'][f\"Dur_{notes[idx][3]}\"],\n",
    "            'dur':0,\n",
    "            'phrase':0,\n",
    "        })\n",
    "        \n",
    "        ## sentence bound\n",
    "        if bound[idx] == '<true>':\n",
    "            tgt_words.append({\n",
    "                'bar':0,\n",
    "                'pos':0,\n",
    "                'token':event2word_dict['Token'][f\"<sent>\"],\n",
    "                'dur':0,\n",
    "                'phrase':0,\n",
    "            })\n",
    "            tgt_sent_id += 1\n",
    "        \n",
    "        '''\n",
    "        tgt_words.append({\n",
    "            'bar':event2word_dict['Bar'][f\"Bar_{notes[idx][0]}\"],\n",
    "            'pos':event2word_dict['Pos'][f\"Pos_{notes[idx][1]}\"],\n",
    "            'token':event2word_dict['Pitch'][f\"Pitch_{notes[idx][2]}\"],\n",
    "            'dur':event2word_dict['Dur'][f\"Dur_{notes[idx][3]}\"],\n",
    "            'phrase':event2word_dict['Phrase'][bound[idx]],\n",
    "        })\n",
    "        '''\n",
    "    \n",
    "    ## eos\n",
    "    tgt_words.append({\n",
    "        'bar':event2word_dict['Token'][f\"</s>\"],\n",
    "        'pos':event2word_dict['Token'][f\"</s>\"],\n",
    "        'token':event2word_dict['Token'][f\"</s>\"],\n",
    "        'dur':event2word_dict['Token'][f\"</s>\"],\n",
    "        'phrase':event2word_dict['Token'][f\"</s>\"],\n",
    "    })\n",
    "    \n",
    "    return src_words, tgt_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "011653b9-dd7f-4128-8401-e6d6c1530c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_binary (midi_pth, i, event2word_dict, split):\n",
    "    try:\n",
    "        src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
    "        if len(src_words) == 0 or len(tgt_words) == 0 or len(tgt_words) > 1024:\n",
    "            return None\n",
    "        \n",
    "        data_sample = {\n",
    "            'input_path': midi_pth,\n",
    "            'item_name': os.path.basename(midi_pth),\n",
    "            'src_words': src_words,\n",
    "            'tgt_words': tgt_words,\n",
    "            'word_length': len(tgt_words)\n",
    "        }\n",
    "        \n",
    "        return [data_sample]\n",
    "    \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "45826a3e-534b-433e-b034-e38a305e43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2binary(dataset_dirs, words_dir, split, word2event_dict, event2word_dict):\n",
    "    # make dir\n",
    "    save_dir = f'{words_dir}/{split}'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    midi_files = []\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        midi_files.extend(glob.glob(os.path.join(os.path.join(dataset_dir, split), \"*.mid\")))\n",
    "    \n",
    "    futures = []\n",
    "    ds_builder = IndexedDatasetBuilder(save_dir)  # index dataset\n",
    "    p = mp.Pool(int(os.getenv('N_PROC', 2)))  # 不要开太大，容易内存溢出\n",
    "    \n",
    "    for i in range (len(midi_files)):\n",
    "        futures.append(p.apply_async(data_to_binary, args=[midi_files[i], i, event2word_dict, split]))\n",
    "    p.close()\n",
    "\n",
    "    words_length = []\n",
    "    all_words = []\n",
    "    for f in tqdm(futures):\n",
    "        item = f.get()\n",
    "        if item is None:\n",
    "            continue\n",
    "        for i in range(len(item)):\n",
    "            sample = item[i]\n",
    "            words_length.append(sample['word_length'])\n",
    "            all_words.append(sample)\n",
    "            ds_builder.add_item(sample) # add item index\n",
    "\n",
    "    # save \n",
    "    ds_builder.finalize()\n",
    "    np.save(f'{words_dir}/{split}_words_length.npy', words_length)\n",
    "    np.save(f'{words_dir}/{split}_words.npy', all_words)\n",
    "    p.join()\n",
    "    print(f'| # {split}_tokens: {sum(words_length)}; {split}_items: {len(words_length)}')\n",
    "    \n",
    "    return all_words, words_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0af7f08d-4645-4223-b8d8-1e4e0b79599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle and split dataset\n",
    "def split_data(output_dir='/home/qihao/CS6207/dataset'):\n",
    "    dataset_dirs = [\n",
    "        '/home/qihao/CS6207/MelodyGLM/MDP/data/processed/pop909/7_dedup',\n",
    "        '/home/qihao/CS6207/MelodyGLM/MDP/data/processed/wikifonia/7_dedup',\n",
    "        '/home/qihao/CS6207/MelodyGLM/MDP/data/processed/mtc/7_dedup',\n",
    "        '/home/qihao/CS6207/MelodyGLM/MDP/data/processed/sessions/7_dedup'\n",
    "    ]\n",
    "    all_files = []\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        all_files.extend(glob.glob(os.path.join(dataset_dir, '*.mid')))\n",
    "    ## shuffle\n",
    "    print(f\"|>>> Total Files: {len(all_files)}\")\n",
    "    \n",
    "    indices = [i for i in range(len(all_files))]\n",
    "    import random, shutil\n",
    "    random.shuffle(indices)\n",
    "    train_end = int(np.floor(0.8*len(all_files)))\n",
    "    valid_end = int(train_end + np.floor(0.1*len(all_files)))\n",
    "    train_idx = indices[:train_end]\n",
    "    valid_idx = indices[train_end:valid_end]\n",
    "    test_idx = indices[valid_end:]\n",
    "    assert len(all_files) == len(train_idx)+len(valid_idx)+len(test_idx)\n",
    "    print(f\"|>>>>> Train Files: {len(train_idx)}\")\n",
    "    print(f\"|>>>>> Valid Files: {len(valid_idx)}\")\n",
    "    print(f\"|>>>>> Test Files: {len(test_idx)}\")\n",
    "    \n",
    "    for split in ['train', 'test', 'valid']:\n",
    "        os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n",
    "    \n",
    "    for t in train_idx:\n",
    "        shutil.copy(all_files[t], os.path.join(f'{output_dir}/train', os.path.basename(all_files[t])))\n",
    "    for v in valid_idx:\n",
    "        shutil.copy(all_files[v], os.path.join(f'{output_dir}/valid', os.path.basename(all_files[v])))\n",
    "    for t in test_idx:\n",
    "        shutil.copy(all_files[t], os.path.join(f'{output_dir}/test', os.path.basename(all_files[t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c978378d-ecfe-4067-9ab4-66360e678137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1d20451a-e662-4d7e-8c79-776a56525208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dirs = ['/home/qihao/CS6207/dataset/']\n",
    "binary_dir = '/home/qihao/CS6207/binary'\n",
    "words_dir = '/home/qihao/CS6207/binary/words'\n",
    "event2word_dict, word2event_dict = pickle.load(open(f\"{binary_dir}/music_dict.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4c9467fd-c6a7-42c0-b929-45841fe3bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 473/473 [00:02<00:00, 234.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| # train_tokens: 222227; train_items: 397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 144.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| # valid_tokens: 27593; valid_items: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████| 60/60 [00:00<00:00, 128.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| # test_tokens: 28047; test_items: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'valid', 'test']:\n",
    "    data2binary(dataset_dirs=dataset_dirs,\n",
    "                words_dir=words_dir,\n",
    "                split=split,\n",
    "                word2event_dict=word2event_dict,\n",
    "                event2word_dict=event2word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edf91b-53b4-4b57-bd30-85b53b8b9a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2287134/3748347528.py\", line 3, in data_to_binary\n",
      "    src_words, tgt_words = tokenise(midi_pth, event2word_dict)\n",
      "  File \"/tmp/ipykernel_2287134/1822061069.py\", line 52, in tokenise\n",
      "    'strength':event2word_dict['Prosody'][f'<sent_{src_sent_id}>'],\n",
      "KeyError: '<sent_72>'\n"
     ]
    }
   ],
   "source": [
    "from miditoolkit.midi.containers import Marker, Instrument, TempoChange, Note\n",
    "def write(words, output_dir, midi_name, word2event):\n",
    "    notes_all = []\n",
    "    markers = []\n",
    "    bar_cnt = -1\n",
    "    positions = 0\n",
    "    midi_obj = miditoolkit.midi.parser.MidiFile()\n",
    "    event_type_list = []\n",
    "    notes_all = []\n",
    "\n",
    "    for event in words:\n",
    "        bar_id, pos_id, pitch_id, dur_id, phrase_id = event[0], event[1], event[2], event[3], event[4]\n",
    "        \n",
    "        bar = word2event['Bar'][bar_id]\n",
    "        pos = word2event['Pos'][pos_id]\n",
    "        pitch = word2event['Pitch'][pitch_id]\n",
    "        dur = word2event['Dur'][dur_id]\n",
    "        phrase = word2event['Phrase'][phrase_id]\n",
    "        \n",
    "        # print(f\"{bar}, {pos}, {pitch}, {dur}, {phrase}\")\n",
    "        \n",
    "        if (\"Bar_\" not in bar) or (\"Pos_\" not in pos) or (\"Pitch_\" not in pitch) or (\"Dur_\" not in dur) or ((\"<true>\" not in phrase) and (\"<false>\" not in phrase)):\n",
    "            continue\n",
    "        bar_num = int(bar.split('_')[1])\n",
    "        pos_num = int(pos.split('_')[1])\n",
    "        pitch_num = int(pitch.split('_')[1])\n",
    "        dur_num = int(dur.split('_')[1])\n",
    "        phrase_bool = True if phrase == '<true>' else False\n",
    "        \n",
    "        start = bar_num*1920 + pos_num\n",
    "        end = start + dur_num\n",
    "        notes_all.append(\n",
    "            Note(pitch=pitch_num, start=start, end=end, velocity=80)\n",
    "        )\n",
    "        if phrase_bool:\n",
    "            markers.append(Marker(time=start, text='Phrase'))\n",
    "        \n",
    "    # tempo\n",
    "    midi_obj.tempo_changes.append(\n",
    "                TempoChange(tempo=65, time=0))\n",
    "\n",
    "    # marker\n",
    "    midi_obj.markers.extend(markers)\n",
    "\n",
    "    # track\n",
    "    piano_track = Instrument(0, is_drum=False, name='melody')\n",
    "\n",
    "    # notes\n",
    "    piano_track.notes = notes_all\n",
    "    midi_obj.instruments = [piano_track]\n",
    "\n",
    "    # save\n",
    "    tgt_melody_pth = os.path.join(output_dir, f\"{midi_name.strip()}.mid\")\n",
    "    \n",
    "    midi_obj.dump(tgt_melody_pth)\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ebf60c36-804f-4e22-9fdb-dc8b7232f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debinarise (data_item, word2event_dict):\n",
    "    out_words = []\n",
    "    for tgt_word in data_item['tgt_words']:\n",
    "        out_words.append((\n",
    "            tgt_word['bar'], tgt_word['pos'], tgt_word['token'], tgt_word['dur'], tgt_word['phrase']\n",
    "        ))\n",
    "    write(words=out_words, \n",
    "          output_dir='/home/qihao/CS6207/debinarise', \n",
    "          midi_name=data_item['item_name'], \n",
    "          word2event=word2event_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "00b2a9ac-435e-486f-aebd-ba3297dc7ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n",
      "/home/qihao/CS6207/debinarise/月满西楼_seg0_1_Seg2.mid.mid\n",
      "/home/qihao/CS6207/debinarise/wikifonia_3009_seg0_1_Seg1.mid.mid\n"
     ]
    }
   ],
   "source": [
    "dataset_test = np.load(\"/home/qihao/CS6207/binary/words/test_words.npy\", allow_pickle=True)\n",
    "print(len(dataset_test))\n",
    "for idx, data_item in enumerate(dataset_test):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    debinarise(data_item, word2event_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fc830-ccd6-48a9-9372-bc389cf95da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e7118-f6ae-4ff7-9197-86b108cd6520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbba8b-d247-4bf2-9dd4-ade01710acfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xailyr)",
   "language": "python",
   "name": "xailyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
