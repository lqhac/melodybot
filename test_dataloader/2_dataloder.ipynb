{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ab1db2-d91b-4965-81e4-cf926ee21ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from utils.task_utils import batchify\n",
    "from template_embedding import TemplateEmbedding\n",
    "from melody_embedding import MelodyEmbedding\n",
    "from utils.indexed_datasets import IndexedDataset\n",
    "from keys import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9396b361-5fd0-44fb-823a-40c52560f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dir = '/home/qihao/CS6207/binary'\n",
    "words_dir = '/home/qihao/CS6207/binary/words'\n",
    "hparams = {\n",
    "    'batch_size': 8,\n",
    "    'word_data_dir': '/home/qihao/CS6207/binary/words',\n",
    "    'sentence_maxlen': 512,\n",
    "    'hidden_size': 768,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247dcc61-8fe5-4353-b15d-3caef20c02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2MDataset (Dataset):\n",
    "    def __init__(self, split, event2word_dict, hparams, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.hparams = hparams\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.event2word_dict = event2word_dict\n",
    "        \n",
    "        self.data_dir = f\"{hparams['word_data_dir']}\"\n",
    "        self.data_path = f'{hparams[\"word_data_dir\"]}/{self.split}_words.npy'\n",
    "        self.ds_name = split ## name of dataset\n",
    "        \n",
    "        self.data = np.load(open(self.data_path, 'rb'), allow_pickle= True)\n",
    "        self.size = np.load(open(f'{hparams[\"word_data_dir\"]}/{self.split}_words_length.npy', 'rb'), allow_pickle= True)\n",
    "        self.shuffle = shuffle\n",
    "        self.sent_maxlen = self.hparams['sentence_maxlen'] ## 512\n",
    "        self.indexed_ds = None ## indexed dataset\n",
    "        self.indices = [] ## indices to data samples\n",
    "        \n",
    "        if shuffle:\n",
    "            self.indices = list(range(len(self.size)))  ## viz. number of data samples\n",
    "            random.shuffle(self.indices)\n",
    "        else:\n",
    "            self.indices = list(range(len(self.size)))\n",
    "    \n",
    "    def ordered_indices(self):\n",
    "        return self.indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def _get_item(self, index):\n",
    "        if self.indexed_ds is None:\n",
    "            self.indexed_ds = IndexedDataset(f'{self.data_dir}/{self.ds_name}')\n",
    "        return self.indexed_ds[index]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # obtain one sentence segment according to the index\n",
    "        item = self._get_item(idx)\n",
    "\n",
    "        # input and output\n",
    "        src_words = item['src_words']\n",
    "        tgt_words = item['tgt_words']\n",
    "        \n",
    "        # to long tensors\n",
    "        for k in src_keys:\n",
    "            item[f'src_{k}'] = torch.LongTensor([word[k] for word in src_words])\n",
    "        for k in tgt_keys:\n",
    "            item[f'tgt_{k}'] = torch.LongTensor([word[k] for word in tgt_words])\n",
    "            \n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def num_workers(self):\n",
    "        return int(os.getenv('NUM_WORKERS',10))  \n",
    "    \n",
    "    def collater(self, samples):\n",
    "        # print(samples)\n",
    "        if len(samples) == 0:\n",
    "            return {}\n",
    "\n",
    "        batch = {}\n",
    "        for k in src_keys:\n",
    "            batch[f'src_{k}'] = batchify([s[f'src_{k}'] for s in samples], pad_idx=0)\n",
    "        for k in tgt_keys:\n",
    "            batch[f'tgt_{k}'] = batchify([s[f'tgt_{k}'] for s in samples], pad_idx=0)\n",
    "        \n",
    "        # batch['n_src_tokens'] = sum([len(s['src_meter']) for s in samples])\n",
    "        # batch['n_tgt_tokens'] = sum([len(s['tgt_word']) for s in samples])\n",
    "        # batch['n_tokens'] = torch.LongTensor([s['n_tokens'] for s in samples])\n",
    "        batch['input_path'] = [s['input_path'] for s in samples]\n",
    "        batch['item_name'] = [s['item_name'] for s in samples]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbe09a1-42bd-47eb-9082-e2610b558e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(dataset, shuffle, batch_size=10, endless=False):\n",
    "    def shuffle_batches(batches):\n",
    "        np.random.shuffle(batches)  # shuffle： 随机打乱数据\n",
    "        return batches\n",
    "\n",
    "    # batch sample and endless\n",
    "    indices = dataset.ordered_indices()\n",
    "\n",
    "    batch_sampler = []\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_sampler.append(indices[i:i + batch_size])  # batch size [0:20],\n",
    "\n",
    "    if shuffle:\n",
    "        batches = shuffle_batches(list(batch_sampler))\n",
    "        if endless:\n",
    "            batches = [b for _ in range(20) for b in shuffle_batches(list(batch_sampler))]\n",
    "    else:\n",
    "        batches = batch_sampler\n",
    "        if endless:\n",
    "            batches = [b for _ in range(20) for b in batches]\n",
    "    \n",
    "    num_workers = dataset.num_workers\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, collate_fn=dataset.collater, num_workers=num_workers,\n",
    "        batch_sampler=batches, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c66c7d-efb7-4db8-94e2-9edd1a573cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    event2word_dict, word2event_dict = pickle.load(open(f\"{binary_dir}/music_dict.pkl\", 'rb'))\n",
    "    batch_size = hparams['batch_size']\n",
    "\n",
    "    test_dataset = L2MDataset('test', event2word_dict, hparams, shuffle=True)\n",
    "    test_dataloader = build_dataloader(dataset=test_dataset, shuffle=True, batch_size=hparams['batch_size'], endless=True)\n",
    "\n",
    "    print(\"length of train_dataloader\", len(test_dataloader))\n",
    "    \n",
    "    # Test embedding\n",
    "    for idx, item in enumerate(tqdm(test_dataloader)):\n",
    "        enc_inputs = {k: item[f'src_{k}'] for k in src_keys}\n",
    "        dec_inputs = {k: item[f'tgt_{k}'] for k in tgt_keys}\n",
    "        template_embed = TemplateEmbedding(event2word_dict=event2word_dict, d_embed=hparams['hidden_size'])\n",
    "        melody_embed = MelodyEmbedding(event2word_dict=event2word_dict, d_embed=hparams['hidden_size'])\n",
    "        enc_emb = template_embed(**enc_inputs)\n",
    "        dec_emb = melody_embed(**dec_inputs)\n",
    "        print(enc_emb.shape, dec_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0dca688-2f3d-44cc-883c-a555556082f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc032268-cd51-48be-b9cc-799eb6216999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xailyr)",
   "language": "python",
   "name": "xailyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
