{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54616499-4aa6-4106-b05f-5b2f8e508329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 20:12:37.535264: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "import torch.cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystopping.protocols import EarlyStopping\n",
    "from test_dataloder import *\n",
    "import datetime\n",
    "from utils.get_time import get_time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from utils.warmup import *\n",
    "import torch.nn.functional as F\n",
    "from transformermodel import *\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e696daa-1c57-4b35-8aad-3c65bde8fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da90aa88-dabb-44e9-8df1-6215d474e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_keys = ['strength', 'length', 'phrase']\n",
    "tgt_keys = ['bar', 'pos', 'token', 'dur', 'phrase']\n",
    "\n",
    "binary_dir = '/home/qihao/CS6207/binary'\n",
    "words_dir = '/home/qihao/CS6207/binary/words'\n",
    "hparams = {\n",
    "    'batch_size': 2,\n",
    "    'word_data_dir': '/home/qihao/CS6207/binary/words',\n",
    "    'sentence_maxlen': 512,\n",
    "    'hidden_size': 256,\n",
    "    'n_layers': 6,\n",
    "    'n_head': 8,\n",
    "    'pretrain': '',\n",
    "    'lr': 5.0e-5,\n",
    "    'optimizer_adam_beta1': 0.9,\n",
    "    'optimizer_adam_beta2': 0.98,\n",
    "    'weight_decay': 0.001,\n",
    "    'patience': 5,\n",
    "    'warmup': 2500,\n",
    "    'lr': 5.0e-5,\n",
    "    'checkpoint_dir': '/home/qihao/CS6207/checkpoints',\n",
    "    'drop_prob': 0.2,\n",
    "    'total_epoch': 1000,\n",
    "    'infer_batch_size': 1,\n",
    "    'temperature': 1.3,\n",
    "    'topk': 5,\n",
    "    'prompt_step': 1,\n",
    "    'infer_max_step': 2048,\n",
    "    'output_dir': \"/home/qihao/CS6207/output_melody\",\n",
    "    'num_heads': 4,\n",
    "    'enc_layers': 4, \n",
    "    'dec_layers': 4, \n",
    "    'enc_ffn_kernel_size': 1,\n",
    "    'dec_ffn_kernel_size': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f197dd9-a209-46c7-96e8-664a52ed6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):  # seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # cuDNN在使用deterministic模式时（下面两行），可能会造成性能下降（取决于model）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5601fb7c-9c8f-4808-8180-c3ba34201b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, device):\n",
    "    \n",
    "    model = Bart(event2word_dict=event2word_dict, \n",
    "                 word2event_dict=word2event_dict, \n",
    "                 model_pth='',\n",
    "                 hidden_size=hparams['hidden_size'], \n",
    "                 num_layers=hparams['n_layers'], \n",
    "                 num_heads=hparams['n_head'], \n",
    "                 dropout=hparams['drop_prob'],).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=True)\n",
    "    model.eval()\n",
    "    print(f\"| Successfully loaded bart ckpt from {checkpoint_path}.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab05e2b0-a7ec-4d65-96be-87178a053308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xe_loss(outputs, targets):\n",
    "    outputs = outputs.transpose(1, 2)\n",
    "    return F.cross_entropy(outputs, targets, ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b780a0-a2b8-4ae8-b51d-d8d3bd5d391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_l2m():\n",
    "    set_seed()\n",
    "    print(f\"Using device: {device} for inferences custom samples\")\n",
    "    \n",
    "    prompt_step = hparams['prompt_step']\n",
    "    \n",
    "    # training conditions (for naming the ckpt)\n",
    "    lr = hparams['lr']\n",
    "\n",
    "    ckpt_dir = '/home/qihao/CS6207/checkpoints/checkpoint_20240330:195754_lr_1e-05'\n",
    "    ckpt_path = os.path.join(ckpt_dir, 'best.pt')\n",
    "\n",
    "    # load dictionary\n",
    "    event2word_dict, word2event_dict = pickle.load(open(f\"{binary_dir}/music_dict.pkl\", 'rb'))\n",
    "    \n",
    "    test_dataset = L2MDataset('test', event2word_dict, hparams, shuffle=False)\n",
    "    test_loader = build_dataloader(dataset=test_dataset, shuffle=True, batch_size=hparams['infer_batch_size'], endless=False)\n",
    "    \n",
    "    print(f\"Test Datalodaer = {len(test_loader)} Songs\")\n",
    "\n",
    "    # load melody generation model based on skeleton framework\n",
    "    model = MusicTransformer(event2word_dict=event2word_dict, \n",
    "                             word2event_dict=word2event_dict, \n",
    "                             hidden_size=hparams['hidden_size'], \n",
    "                             num_heads=hparams['num_heads'],\n",
    "                             enc_layers=hparams['enc_layers'], \n",
    "                             dec_layers=hparams['dec_layers'], \n",
    "                             dropout=hparams['drop_prob'], \n",
    "                             enc_ffn_kernel_size=hparams['enc_ffn_kernel_size'],\n",
    "                             dec_ffn_kernel_size=hparams['dec_ffn_kernel_size'],\n",
    "                            ).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=True)\n",
    "    model.eval()\n",
    "    print(f\"| Successfully loaded bart ckpt from {ckpt_path}.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Inference file path\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    exp_date = get_time()\n",
    "    melody_output_dir = os.path.join(hparams['output_dir'], f'melody_{exp_date}')\n",
    "    if not os.path.exists(melody_output_dir):\n",
    "        os.mkdir(melody_output_dir)\n",
    "    \n",
    "    for data_idx, data in enumerate(test_loader):\n",
    "        try:\n",
    "            # print(data[f'tgt_bar'])\n",
    "            # print(data[f'tgt_pos'])\n",
    "            # print(data[f'tgt_token'])\n",
    "            # print(data[f'tgt_dur'])\n",
    "            # print(data[f'tgt_phrase'])\n",
    "            data_name = data['item_name'][0] if '.mid' not in data['item_name'][0] else data['item_name'][0][:-4]\n",
    "            # print(data['item_name'])\n",
    "\n",
    "            enc_inputs = {k: data[f'src_{k}'].to(device) for k in src_keys}\n",
    "            dec_inputs = {k: data[f'tgt_{k}'].to(device) for k in tgt_keys}\n",
    "            \n",
    "            dec_inputs_selected = {\n",
    "                'bar': dec_inputs['bar'][:, :prompt_step],\n",
    "                'pos': dec_inputs['pos'][:, :prompt_step],\n",
    "                'token': dec_inputs['token'][:, :prompt_step],\n",
    "                'dur': dec_inputs['dur'][:, :prompt_step],\n",
    "                'phrase': dec_inputs['phrase'][:, :prompt_step],\n",
    "            }\n",
    "            \n",
    "            _ = model.infer(enc_inputs=enc_inputs, \n",
    "                            dec_inputs_gt=dec_inputs_selected, \n",
    "                            sentence_maxlen=hparams['infer_max_step'], \n",
    "                            temperature=hparams['temperature'], \n",
    "                            topk=hparams['topk'], \n",
    "                            device=device, \n",
    "                            output_dir=melody_output_dir, \n",
    "                            midi_name=data_name)\n",
    "            \n",
    "            print(f\"Generating {data_idx+1}/{len(test_loader)}, Name: {data_name}\")\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-\\nBad Item: {data_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdca57e4-ea82-46a9-93b3-85c50395b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1 for inferences custom samples\n",
      "Test Datalodaer = 54 Songs\n",
      "| Successfully loaded bart ckpt from /home/qihao/CS6207/checkpoints/checkpoint_20240330:195754_lr_1e-05/best.pt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|                                                  | 0/2048 [00:00<?, ?it/s]/home/qihao/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████▉| 2047/2048 [00:16<00:00, 124.90it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  1%|▎                                       | 13/2048 [00:00<00:15, 127.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qihao/CS6207/output_melody/melody_20240330:201240/最重要的决定_seg0_1_Seg1.mid\n",
      "Generating 1/54, Name: 最重要的决定_seg0_1_Seg1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████▉| 2047/2048 [00:15<00:00, 132.63it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  1%|▎                                       | 14/2048 [00:00<00:15, 132.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qihao/CS6207/output_melody/melody_20240330:201240/菊花台_seg0_1_Seg1.mid\n",
      "Generating 2/54, Name: 菊花台_seg0_1_Seg1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▊                                | 357/2048 [00:02<00:12, 132.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minfer_l2m\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 65\u001b[0m, in \u001b[0;36minfer_l2m\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     dec_inputs \u001b[38;5;241m=\u001b[39m {k: data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tgt_keys}\n\u001b[1;32m     57\u001b[0m     dec_inputs_selected \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m: dec_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :prompt_step],\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m: dec_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :prompt_step],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m: dec_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :prompt_step],\n\u001b[1;32m     63\u001b[0m     }\n\u001b[0;32m---> 65\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdec_inputs_gt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_inputs_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msentence_maxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minfer_max_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmelody_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmidi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/CS6207/transformermodel.py:216\u001b[0m, in \u001b[0;36mMusicTransformer.infer\u001b[0;34m(self, enc_inputs, dec_inputs_gt, sentence_maxlen, temperature, topk, device, output_dir, midi_name, tf_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m tgt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_emb(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v[:, step:step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dec_inputs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m    215\u001b[0m enc_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(cond_embeds)\n\u001b[0;32m--> 216\u001b[0m dec_outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m token_out \u001b[38;5;241m=\u001b[39m dec_outputs\n\u001b[1;32m    220\u001b[0m token_logits \u001b[38;5;241m=\u001b[39m token_out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CS6207/modules/mumidi_transformer/mumidi_transformer.py:742\u001b[0m, in \u001b[0;36mMusicTransformerDecoder.forward\u001b[0;34m(self, dec_inputs, encoder_out, incremental_state)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m         self_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 742\u001b[0m     x, attn_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincremental_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincremental_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     all_attn_logits\u001b[38;5;241m.\u001b[39mappend(attn_logits)\n\u001b[1;32m    752\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CS6207/modules/mumidi_transformer/mumidi_transformer.py:503\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CS6207/modules/mumidi_transformer/mumidi_transformer.py:676\u001b[0m, in \u001b[0;36mDecSALayer.forward\u001b[0;34m(self, x, encoder_out, encoder_padding_mask, incremental_state, self_attn_mask, self_attn_padding_mask, attn_out, reset_attn_weight, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    675\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm3(x)\n\u001b[0;32m--> 676\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincremental_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    678\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CS6207/modules/mumidi_transformer/mumidi_transformer.py:567\u001b[0m, in \u001b[0;36mTransformerFFNLayer.forward\u001b[0;34m(self, x, incremental_state)\u001b[0m\n\u001b[1;32m    564\u001b[0m     saved_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_input_buffer(incremental_state, saved_state)\n\u001b[0;32m--> 567\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    568\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incremental_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer_l2m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76466b-4442-4885-961b-15ef8393f4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xailyr)",
   "language": "python",
   "name": "xailyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
