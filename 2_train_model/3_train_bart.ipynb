{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54616499-4aa6-4106-b05f-5b2f8e508329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "import torch.cuda\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystopping.protocols import EarlyStopping\n",
    "from test_dataloder import *\n",
    "import datetime\n",
    "from utils.get_time import get_time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from utils.warmup import *\n",
    "import torch.nn.functional as F\n",
    "from bartmodel import Bart\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da90aa88-dabb-44e9-8df1-6215d474e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_keys = ['strength', 'length', 'phrase']\n",
    "tgt_keys = ['bar', 'pos', 'token', 'dur', 'phrase']\n",
    "\n",
    "binary_dir = '/home/qihao/CS6207/binary'\n",
    "words_dir = '/home/qihao/CS6207/binary/words'\n",
    "hparams = {\n",
    "    'batch_size': 8,\n",
    "    'word_data_dir': '/home/qihao/CS6207/binary/words',\n",
    "    'sentence_maxlen': 512,\n",
    "    'hidden_size': 768,\n",
    "    'n_layers': 6,\n",
    "    'n_head': 8,\n",
    "    'pretrain': '',\n",
    "    'lr': 1.0e-5,\n",
    "    'optimizer_adam_beta1': 0.9,\n",
    "    'optimizer_adam_beta2': 0.98,\n",
    "    'weight_decay': 0.01,\n",
    "    'patience': 5,\n",
    "    'warmup': 3000,\n",
    "    'lr': 5.0e-5,\n",
    "    'checkpoint_dir': '/home/qihao/CS6207/checkpoints',\n",
    "    'drop_prob': 0.2,\n",
    "    'total_epoch': 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f197dd9-a209-46c7-96e8-664a52ed6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):  # seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # cuDNN在使用deterministic模式时（下面两行），可能会造成性能下降（取决于model）\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab05e2b0-a7ec-4d65-96be-87178a053308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xe_loss(outputs, targets):\n",
    "    outputs = outputs.transpose(1, 2)\n",
    "    return F.cross_entropy(outputs, targets, ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa1fc38f-2432-4ab4-a717-d162972ede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, epoch, total_epoch):\n",
    "    # define the format of tqdm\n",
    "    with tqdm(total=len(train_loader), ncols=150, position=0, leave=True) as _tqdm:  # 总长度是data的长度\n",
    "        _tqdm.set_description('training epoch: {}/{}'.format(epoch + 1, total_epoch))  # 设置前缀更新信息\n",
    "\n",
    "        # Model Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_loss = []\n",
    "        train_bar_loss = []\n",
    "        train_pos_loss = []\n",
    "        train_token_loss = []\n",
    "        train_dur_loss = []\n",
    "        train_phrase_loss = []\n",
    "\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            # prompt_index = list(data[f'tgt_word'].numpy()).index(50268)\n",
    "            enc_inputs = {k: data[f'src_{k}'].to(device) for k in src_keys}\n",
    "            dec_inputs = {k: data[f'tgt_{k}'].to(device) for k in tgt_keys}\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            bar_out, pos_out, token_out, dur_out, phrase_out = model(enc_inputs, dec_inputs)\n",
    "            \n",
    "            # print(bar_out, bar_out.logit())\n",
    "            \n",
    "            bar_out = bar_out #.logit()\n",
    "            tgt_bar = (data['tgt_bar'].to(device))[:, 1:]\n",
    "            bar_loss = xe_loss(bar_out[:, :-1], tgt_bar)\n",
    "            \n",
    "            pos_out = pos_out #.logit()\n",
    "            tgt_pos = (data['tgt_pos'].to(device))[:, 1:]\n",
    "            pos_loss = xe_loss(pos_out[:, :-1], tgt_pos)\n",
    "            \n",
    "            token_out = token_out #.logit()\n",
    "            tgt_token = (data['tgt_token'].to(device))[:, 1:]\n",
    "            token_loss = xe_loss(token_out[:, :-1], tgt_token)\n",
    "            \n",
    "            dur_out = dur_out #.logit()\n",
    "            tgt_dur = (data['tgt_dur'].to(device))[:, 1:]\n",
    "            dur_loss = xe_loss(dur_out[:, :-1], tgt_dur)\n",
    "            \n",
    "            phrase_out = phrase_out #.logit()\n",
    "            tgt_phrase = (data['tgt_phrase'].to(device))[:, 1:]\n",
    "            phrase_loss = xe_loss(phrase_out[:, :-1], tgt_phrase)\n",
    "            \n",
    "\n",
    "            # 3) total loss\n",
    "            total_loss = bar_loss + pos_loss + token_loss + dur_loss + phrase_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss.append(total_loss.item())\n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "            train_bar_loss.append(bar_loss.item())\n",
    "            train_pos_loss.append(pos_loss.item())\n",
    "            train_token_loss.append(token_loss.item())\n",
    "            train_dur_loss.append(dur_loss.item())\n",
    "            train_phrase_loss.append(phrase_loss.item())\n",
    "\n",
    "            _tqdm.set_postfix(\n",
    "                loss=\"{:.3f}, bar={:.3f}, pos={:.3f}, token={:.3f}, dur={:.3f}, phrase={:.3f}\".format(total_loss,\n",
    "                                                                                                      bar_loss, \n",
    "                                                                                                      pos_loss,\n",
    "                                                                                                      token_loss,\n",
    "                                                                                                      dur_loss,\n",
    "                                                                                                      phrase_loss))\n",
    "            \n",
    "            _tqdm.update(2)\n",
    "\n",
    "    train_loss_avg = np.mean(train_loss)\n",
    "    train_bar_loss_avg = np.mean(train_bar_loss)\n",
    "    train_pos_loss_avg = np.mean(train_pos_loss)\n",
    "    train_token_loss_avg = np.mean(train_token_loss)\n",
    "    train_dur_loss_avg = np.mean(train_dur_loss)\n",
    "    train_phrase_loss_avg = np.mean(train_phrase_loss)\n",
    "    \n",
    "    return train_loss_avg, train_bar_loss_avg, train_pos_loss_avg, train_token_loss_avg, train_dur_loss_avg, train_phrase_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56ba69f8-9040-4096-ac6b-b829563fa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(valid_loader, model, epoch, total_epoch):\n",
    "    # define the format of tqdm\n",
    "    with tqdm(total=len(valid_loader), ncols=150) as _tqdm:  # 总长度是data的长度\n",
    "        _tqdm.set_description('validation epoch: {}/{}'.format(epoch + 1, total_epoch))  # 设置前缀更新信息\n",
    "\n",
    "        model.eval()  # switch to valid mode\n",
    "        running_loss = 0.0\n",
    "        val_loss = []\n",
    "        val_bar_loss = []\n",
    "        val_pos_loss = []\n",
    "        val_token_loss = []\n",
    "        val_dur_loss = []\n",
    "        val_phrase_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate((valid_loader)):\n",
    "                try:\n",
    "                    enc_inputs = {k: data[f'src_{k}'].to(device) for k in src_keys}\n",
    "                    dec_inputs = {k: data[f'tgt_{k}'].to(device) for k in tgt_keys}\n",
    "\n",
    "                    bar_out, pos_out, token_out, dur_out, phrase_out = model(enc_inputs, dec_inputs)\n",
    "\n",
    "                    bar_out = bar_out #.logit()\n",
    "                    tgt_bar = (data['tgt_bar'].to(device))[:, 1:]\n",
    "                    bar_loss = xe_loss(bar_out[:, :-1], tgt_bar)\n",
    "\n",
    "                    pos_out = pos_out #.logit()\n",
    "                    tgt_pos = (data['tgt_pos'].to(device))[:, 1:]\n",
    "                    pos_loss = xe_loss(pos_out[:, :-1], tgt_pos)\n",
    "\n",
    "                    token_out = token_out #.logit()\n",
    "                    tgt_token = (data['tgt_token'].to(device))[:, 1:]\n",
    "                    token_loss = xe_loss(token_out[:, :-1], tgt_token)\n",
    "\n",
    "                    dur_out = dur_out #.logit()\n",
    "                    tgt_dur = (data['tgt_dur'].to(device))[:, 1:]\n",
    "                    dur_loss = xe_loss(dur_out[:, :-1], tgt_dur)\n",
    "\n",
    "                    phrase_out = phrase_out# .logit()\n",
    "                    tgt_phrase = (data['tgt_phrase'].to(device))[:, 1:]\n",
    "                    phrase_loss = xe_loss(phrase_out[:, :-1], tgt_phrase)\n",
    "\n",
    "\n",
    "                    # 3) total loss\n",
    "                    total_loss = bar_loss + pos_loss + token_loss + dur_loss + phrase_loss\n",
    "                    val_loss.append(total_loss.item())\n",
    "                    running_loss += total_loss.item()\n",
    "\n",
    "                    val_bar_loss.append(bar_loss.item())\n",
    "                    val_pos_loss.append(pos_loss.item())\n",
    "                    val_token_loss.append(token_loss.item())\n",
    "                    val_dur_loss.append(dur_loss.item())\n",
    "                    val_phrase_loss.append(phrase_loss.item())\n",
    "\n",
    "                    _tqdm.set_postfix(\n",
    "                        loss=\"{:.3f}, bar={:.3f}, pos={:.3f}, token={:.3f}, dur={:.3f}, phrase={:.3f}\".format(total_loss,\n",
    "                                                                                                              bar_loss, \n",
    "                                                                                                              pos_loss,\n",
    "                                                                                                              token_loss,\n",
    "                                                                                                              dur_loss,\n",
    "                                                                                                              phrase_loss))\n",
    "\n",
    "                    _tqdm.update(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(data)\n",
    "                    print(\"Bad Data Item!\")\n",
    "                    print(e)\n",
    "                    break\n",
    "            \n",
    "    val_loss_avg = np.mean(val_loss)\n",
    "    val_bar_loss_avg = np.mean(val_bar_loss)\n",
    "    val_pos_loss_avg = np.mean(val_pos_loss)\n",
    "    val_token_loss_avg = np.mean(val_token_loss)\n",
    "    val_dur_loss_avg = np.mean(val_dur_loss)\n",
    "    val_phrase_loss_avg = np.mean(val_phrase_loss)\n",
    "\n",
    "    return val_loss_avg, val_bar_loss_avg, val_pos_loss_avg, val_token_loss_avg, val_dur_loss_avg, val_phrase_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b780a0-a2b8-4ae8-b51d-d8d3bd5d391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l2m():\n",
    "    ## train melody to lyric generation\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    global device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # args\n",
    "    set_seed()\n",
    "    # set_hparams()\n",
    "    event2word_dict, word2event_dict = pickle.load(open(f\"{binary_dir}/music_dict.pkl\", 'rb'))\n",
    "\n",
    "    # tensorboard logger\n",
    "    cur_time = get_time()\n",
    "    # tensorboard_dir = hparams['tensorboard']\n",
    "    # train_log_dir = f'{tensorboard_dir}/{cur_time}/train'\n",
    "    # valid_log_dir = f'{tensorboard_dir}/{cur_time}/valid'\n",
    "    # train_writer = SummaryWriter(log_dir=train_log_dir)\n",
    "    # valid_writer = SummaryWriter(log_dir=valid_log_dir)\n",
    "\n",
    "    # ------------\n",
    "    # train\n",
    "    # ------------\n",
    "    # load data\n",
    "    train_dataset = L2MDataset('train', event2word_dict, hparams, shuffle=True)\n",
    "    valid_dataset = L2MDataset('valid', event2word_dict, hparams, shuffle=False)\n",
    "\n",
    "    train_loader = build_dataloader(dataset=train_dataset, shuffle=True, batch_size=hparams['batch_size'], endless=False)\n",
    "    val_loader = build_dataloader(dataset=valid_dataset, shuffle=False, batch_size=hparams['batch_size'], endless=False)\n",
    "    \n",
    "    print(len(train_loader))\n",
    "    \n",
    "    # print(f\"foundation model pth: {hparams['custom_model_dir']}\")\n",
    "    \n",
    "    def tensor_check_fn(key, param, input_param, error_msgs):\n",
    "        if param.shape != input_param.shape:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    model = Bart(event2word_dict=event2word_dict, \n",
    "                 word2event_dict=word2event_dict, \n",
    "                 model_pth='',\n",
    "                 hidden_size=hparams['hidden_size'], \n",
    "                 num_layers=hparams['n_layers'], \n",
    "                 num_heads=hparams['n_head'], \n",
    "                 dropout=hparams['drop_prob'],).to(device)\n",
    "    \n",
    "    pre_trained_path = hparams['pretrain']\n",
    "    if pre_trained_path != '':\n",
    "        current_model_dict = model.state_dict()\n",
    "        loaded_state_dict = torch.load(pre_trained_path)\n",
    "        new_state_dict={k:v if v.size()==current_model_dict[k].size() else current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}\n",
    "        # model.load_state_dict(new_state_dict, strict=False)\n",
    "        # model.load_state_dict(torch.load(pre_trained_path), strict=False, tensor_check_fn=tensor_check_fn)\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        print(\">>> Load pretrained model successfully\")\n",
    "        \n",
    "    ## warm up\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hparams['lr'],\n",
    "        betas=(hparams['optimizer_adam_beta1'], hparams['optimizer_adam_beta2']),\n",
    "        weight_decay=hparams['weight_decay'])\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=hparams['warmup'], num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \"\"\"\n",
    "\n",
    "    # training conditions (for naming the ckpt)\n",
    "    lr = hparams['lr']\n",
    "\n",
    "    # early stop: initialize the early_stopping object\n",
    "    # checkpointpath = f\"{hparams['checkpoint_dir']}/Cond_{cond}_GPT2_{cur_time}_lr{lr}\"\n",
    "    checkpointpath = f\"{hparams['checkpoint_dir']}/checkpoint_{cur_time}_lr_{lr}\"\n",
    "    if not os.path.exists(checkpointpath):\n",
    "        os.mkdir(checkpointpath)\n",
    "    early_stopping = EarlyStopping(patience=hparams['patience'], verbose=True,\n",
    "                                   path=f\"{checkpointpath}/early_stopping_checkpoint.pt\")\n",
    "    \n",
    "\n",
    "    # -------- Train & Validation -------- #\n",
    "    min_valid_running_loss = 1000000  # inf\n",
    "    total_epoch = hparams['total_epoch']\n",
    "    with tqdm(total=total_epoch) as _tqdm:\n",
    "        for epoch in range(total_epoch):\n",
    "            # Train\n",
    "            train_running_loss, _, _, _, _, _ = train(train_loader, model, optimizer, scheduler, epoch, total_epoch)\n",
    "            # train_writer.add_scalars(\"train_epoch_loss\", {\"running\": train_running_loss, 'word': train_word_loss}, epoch)\n",
    "\n",
    "            # validation  \n",
    "            valid_running_loss, _, _, _, _, _ = valid(val_loader, model, epoch, total_epoch)\n",
    "            # valid_writer.add_scalars(\"valid_epoch_loss\", {\"running\": valid_running_loss, 'word': valid_word_loss}, epoch)\n",
    "\n",
    "            # early stopping Check\n",
    "            early_stopping(valid_running_loss, model, epoch)\n",
    "            if early_stopping.early_stop == True:\n",
    "                print(\"Validation Loss convergence， Train over\")\n",
    "                break\n",
    "\n",
    "            # save the best checkpoint\n",
    "            if valid_running_loss < min_valid_running_loss:\n",
    "                min_valid_running_loss = valid_running_loss\n",
    "                torch.save(model.state_dict(), f\"{checkpointpath}/best.pt\")\n",
    "            print(f\"Training Runinng Loss = {train_running_loss}, Validation Running Loss = {min_valid_running_loss}\")  \n",
    "            _tqdm.update(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdca57e4-ea82-46a9-93b3-85c50395b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/1000:   0%|               | 6/1545 [00:00<02:03, 12.45it/s, loss=23.398, bar=5.553, pos=4.769, token=4.894, dur=5.664, phrase=2.518]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 64, 66, 61, 64, 66, 64, 66, 61, 64, 69, 69, 69, 68, 66, 61, 64, 66,\n",
      "         61, 64, 66, 64, 66, 61, 64, 59, 61, 63, 64, 66, 71, 68, 64, 64, 66, 61,\n",
      "         64, 66, 64, 66, 61, 64, 64, 66, 66, 68, 68, 69, 66, 68, 69, 70, 71, 64,\n",
      "         73, 71, 69, 64, 73, 73, 69, 64, 67, 73, 71, 69, 66, 71, 69, 66, 65, 71,\n",
      "         69, 65, 64, 73, 73, 71, 69, 66, 74, 73, 64, 66,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 52, 54, 56, 51, 52, 54, 56, 53, 54, 56, 57, 59, 56, 59, 54, 56, 57,\n",
      "         59, 57, 59, 61, 63, 64, 66, 68, 68, 68, 64, 61, 64, 64, 66, 66, 63, 59,\n",
      "         63, 64, 64, 61, 57, 61, 61, 54, 56, 57, 59, 54, 52, 54, 56, 51, 52, 54,\n",
      "         56, 53, 54, 56, 57, 59, 56, 59, 54, 56, 57, 59, 57, 59, 61, 63, 64, 66,\n",
      "         68, 68, 68, 64, 61, 64, 64, 66, 66, 63, 59, 63, 64, 64, 61, 57, 61, 61,\n",
      "         54, 56, 57, 56, 52, 51, 52, 54, 56, 56, 56, 56, 56, 56, 56, 56, 58, 60,\n",
      "         55, 56, 58, 60, 60, 60, 60, 60, 60, 61, 63, 60, 61, 63, 52, 54, 56, 51,\n",
      "         52, 54, 56, 53, 54, 56, 57, 59, 56, 59, 54, 56, 57, 59, 57, 59, 61, 63,\n",
      "         64, 66, 68, 68, 68, 64, 61, 64, 64, 64, 66, 68, 64, 60, 64, 64, 66, 68,\n",
      "         63, 64, 66, 68, 63, 64, 51, 52,  2]])\n",
      "tensor([[ 1, 64, 66, 68, 68, 68, 64, 66, 66, 68, 66, 64, 64, 66, 68, 68, 66, 64,\n",
      "         66, 64, 64, 61, 64, 64, 66, 68, 68, 66, 64, 66, 66, 66, 68, 66, 64, 66,\n",
      "         68, 68, 66, 64, 66, 64, 64, 61, 64, 64, 66, 68, 68, 68, 64, 66, 66, 64,\n",
      "         63, 64, 68, 69, 71, 68, 69, 71, 66, 68, 69, 69, 68, 66, 68, 69, 68, 69,\n",
      "         71, 71, 71, 73, 68, 66, 64, 66, 64, 64, 61, 64, 64, 66, 68, 68, 68, 68,\n",
      "         66, 66, 68, 66, 64, 76, 78, 80, 80, 80, 76, 78, 78, 80,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 59, 59, 59, 64, 64, 61, 57, 54, 61, 61, 59, 63, 61, 57, 56, 59, 68,\n",
      "         66, 64, 61, 57, 54, 61, 59, 64, 63, 66, 64, 68, 68, 69, 69, 66, 66, 68,\n",
      "         68, 64, 64, 66, 64, 63, 61, 59, 68, 66, 64, 61, 57, 54, 61, 59, 64, 63,\n",
      "         66, 64, 64, 64, 64, 66, 68, 69, 64, 61, 57, 57, 57, 59, 59, 59, 61, 63,\n",
      "         64, 68, 66, 64, 61, 59, 64, 64, 64, 66, 68, 69, 64, 61, 57, 57, 57, 59,\n",
      "         59, 59, 61, 63, 64, 68, 66, 64, 68, 68, 69, 68, 66, 64, 66, 68, 66, 64,\n",
      "         68, 68, 69, 71, 69, 68, 69, 71, 69, 66, 71, 71, 73, 71, 69, 68, 69, 64,\n",
      "         66, 68, 64, 66, 68, 68, 68, 68, 69, 68, 66,  2]])\n",
      "tensor([[ 1, 71, 71, 71, 71, 71, 73, 73, 73, 73, 73, 75, 75, 75, 75, 73, 75, 73,\n",
      "         75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 78, 78, 78, 78, 76, 75, 73, 79,\n",
      "         80, 79, 80, 79, 80, 81, 76, 80, 78, 71, 76, 75, 76, 75, 76, 80, 78, 76,\n",
      "         75, 73, 72, 71, 71, 71, 71, 71, 73, 73, 73, 73, 73, 81, 81, 81, 81, 78,\n",
      "         80,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 69, 68, 69, 68, 69, 71, 73, 75, 75, 73, 71, 73, 68, 68, 69, 68, 69,\n",
      "         71, 73, 71, 73, 75, 73, 71, 73, 80, 80, 80, 76, 80, 78, 76, 79, 78, 76,\n",
      "         71, 68, 69, 68, 69, 71, 73, 79, 79, 78, 76, 73, 76, 68, 69, 68, 69, 71,\n",
      "         73, 73, 75, 75, 73, 71, 73, 68, 68, 69, 68, 69, 71, 73, 71, 73, 75, 73,\n",
      "         71, 73, 80, 80, 80, 76, 80, 78, 76, 79, 78, 76, 71, 68, 69, 68, 69, 71,\n",
      "         73, 79, 79, 78, 76, 73, 76, 76, 78, 76, 78, 76, 79, 76, 78, 76, 80, 78,\n",
      "         76, 73, 71, 76, 78, 76, 78, 76, 79, 76, 78, 76, 80, 78, 76, 73, 71, 68,\n",
      "         69, 68, 69, 71, 73, 75, 75, 73, 71, 73, 68, 68, 69, 68, 69, 71, 73, 73,\n",
      "         71, 73, 75, 73, 71, 73, 80, 80, 80, 76, 80, 78, 76, 79, 78, 76, 71, 68,\n",
      "         69, 68, 69, 71, 73, 79, 79, 78, 76,  2]])\n",
      "tensor([[ 1, 76, 71, 71, 73, 68, 71, 68, 71, 68, 64, 61, 59, 59, 64, 64, 68, 68,\n",
      "         71, 71, 73, 73, 71, 76, 71, 68, 66, 64, 61, 64, 64, 64, 64, 69, 69, 73,\n",
      "         76, 76, 73, 71, 71, 64, 61, 64, 64, 59, 59, 64, 64, 61, 64, 59, 64, 69,\n",
      "         69, 73, 76, 73, 71, 71, 68, 69, 71, 68, 66, 64, 61, 64, 76, 71, 71, 73,\n",
      "         68, 71, 68, 71, 68, 64, 61, 59, 59, 64, 64, 68, 68, 71, 71, 73, 73, 71,\n",
      "         76, 71, 68, 64,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 68, 68, 66, 64, 66, 68, 68, 66, 64, 66, 68, 68, 66, 64, 66, 64, 64,\n",
      "         73, 73, 73, 71, 69, 71, 73, 73, 71, 69, 71, 69, 69, 73, 73, 71, 71, 73,\n",
      "         67, 66, 64, 61, 59, 61, 64, 67, 66, 64, 59, 61, 64, 67, 66, 64, 61, 61,\n",
      "         59, 61, 64, 67, 66, 64, 61, 64, 61, 64, 64, 64, 64, 63, 61, 59, 61, 63,\n",
      "         63, 62, 62, 62, 62, 64, 66, 64, 64, 64, 66, 66, 66, 66, 65, 63, 61, 63,\n",
      "         68, 65, 61, 63, 64, 64, 59, 61, 63, 63, 68, 68, 68, 66, 64, 66, 68, 68,\n",
      "         66, 64, 66, 68, 68, 66, 64, 66, 64, 64, 73, 73, 71, 69, 71, 73, 73, 71,\n",
      "         69, 71, 69, 69, 71, 71, 71, 71, 73, 67, 66, 64, 59, 61, 64, 67, 66, 64,\n",
      "         64, 59, 61, 64, 67, 66, 64, 61,  2]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/1000:   1%|▏             | 14/1545 [00:00<01:08, 22.23it/s, loss=23.475, bar=5.504, pos=4.768, token=4.998, dur=5.579, phrase=2.626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 72, 72, 72, 72, 64, 66, 64, 61, 73, 73, 73, 73, 73, 73, 64, 66, 64,\n",
      "         61, 71, 68, 68, 68, 68, 64, 68, 64, 68, 64, 68, 64, 68, 61, 68, 66, 64,\n",
      "         64, 66, 64, 66, 64, 63, 61, 68, 68, 68, 68, 64, 66, 68, 66, 64, 68, 64,\n",
      "         68, 61, 68, 66, 64, 64, 66, 64, 63, 61, 60, 60, 63, 61, 68, 67, 66, 66,\n",
      "         64, 63, 61,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 66, 68, 64, 61, 64, 66, 66, 66, 68, 64, 61, 64, 68, 68, 66, 68, 66,\n",
      "         64, 64, 71, 71, 59, 68, 66, 64, 59, 68, 66, 66, 64, 66, 71, 73, 71, 68,\n",
      "         66, 68, 71, 73, 71, 68, 64, 66, 71, 73, 71, 66, 63, 64, 63, 64, 66, 68,\n",
      "         66, 68, 69, 71, 71, 71, 73, 66, 68, 64, 71, 71, 71, 66, 68, 64, 68, 68,\n",
      "         68, 66, 64, 66, 66, 66, 68, 69, 68, 66, 64, 66, 71, 73, 71, 68, 66, 68,\n",
      "         71, 73, 71, 68, 64, 66, 71, 73, 71, 66, 63, 64, 63, 64, 66, 68, 66, 68,\n",
      "         75, 71, 71, 71, 73, 66, 68, 64, 71, 71, 71, 66, 68, 64, 68, 68, 68, 66,\n",
      "         64, 66, 66, 68, 69, 68, 66, 64, 66, 59, 68, 66, 64,  2]])\n",
      "tensor([[ 1, 61, 61, 63, 64, 68, 66, 64, 63, 64, 63, 61, 61, 59, 61, 61, 61, 61,\n",
      "         63, 64, 68, 66, 64, 63, 64, 63, 64, 61, 61, 59, 68, 69, 68, 66, 64, 69,\n",
      "         69, 69, 68, 69, 68, 69, 68, 66, 64, 66, 73, 71, 66, 68, 66, 64, 61, 63,\n",
      "         64, 61, 63, 64, 66, 64, 64, 66, 68, 69, 68, 71, 68, 76, 73, 71, 73, 68,\n",
      "         66, 66, 68, 75, 71, 66, 66, 68, 66, 64, 61, 64, 68, 66, 64, 66, 64, 61,\n",
      "         73, 71, 71, 68, 59, 68, 71, 68, 76, 73, 71, 73, 68, 66, 66, 68, 75, 71,\n",
      "         66, 68, 66, 64, 61, 64, 68, 66, 66, 64, 66, 64, 61, 73, 71, 68, 64, 61,\n",
      "         66, 64, 71, 68, 76, 73, 71, 73, 68, 66, 66, 68, 75, 71, 66, 66, 68, 66,\n",
      "         64, 61, 64, 68, 66, 64, 66, 64, 61, 73, 71, 73, 71, 66, 68, 71, 68, 76,\n",
      "         73, 71, 73, 68, 66, 66, 68, 75, 71, 66, 68, 66, 64, 61, 64, 68, 66, 64,\n",
      "         66, 64, 61, 73, 71, 68, 64, 61,  2],\n",
      "        [ 1, 59, 59, 64, 64, 66, 66, 68, 68, 66, 64, 66, 59, 61, 63, 64, 64, 63,\n",
      "         61, 59, 52, 63, 64, 61, 61, 59, 57, 59, 59, 57, 56, 59, 57, 49, 57, 56,\n",
      "         56, 54, 59, 59, 64, 64, 66, 66, 68, 68, 66, 64, 66, 59, 61, 63, 64, 64,\n",
      "         64, 63, 61, 59, 52, 63, 64, 61, 61, 63, 64, 66, 66, 64, 59, 59, 68, 69,\n",
      "         66, 59, 68, 69, 68, 66, 68, 68, 68, 69, 69, 68, 66, 68, 68, 68, 69, 69,\n",
      "         68, 66, 68, 68, 68, 69, 71, 64, 63, 64, 66, 64, 66, 68, 69, 68, 66, 64,\n",
      "         61, 61, 71, 71, 69, 68, 69, 59, 59, 71, 69, 68, 66, 68, 61, 61, 66, 66,\n",
      "         68, 68, 69, 61, 66, 68, 69, 68, 66, 64,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[ 1, 64, 64, 64, 66, 68, 71, 68, 66, 64, 61, 64, 64, 64, 66, 68, 71, 68,\n",
      "         66, 64, 66, 73, 73, 73, 73, 71, 73, 76, 73, 73, 71, 71, 68, 66, 64, 66,\n",
      "         66, 66, 66, 68, 69, 76, 73, 71, 73, 71, 80, 80, 80, 80, 80, 78, 76, 78,\n",
      "         76, 73, 78, 78, 78, 78, 76, 80, 78, 76, 73, 76, 78, 80, 80, 80, 80, 80,\n",
      "         78, 80, 78, 80, 83, 80, 78, 76, 73, 78, 78, 78, 78, 76, 80, 78, 78, 76,\n",
      "         73, 76, 73, 71, 80, 80, 80, 80, 80, 78, 76, 78, 76, 73, 78, 78, 78, 78,\n",
      "         76, 80, 78, 76, 73, 76, 78, 80, 80, 80, 80, 80, 78, 80, 78, 80, 83, 80,\n",
      "         78, 76, 73, 78, 78, 78, 78, 76, 80, 78, 76, 73, 76, 76, 78, 78, 78, 78,\n",
      "         76, 80, 78, 76, 73, 76,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 78, 78, 77, 78, 78, 77, 78, 77, 78, 78, 77, 78, 78, 76, 78, 78, 76,\n",
      "         78, 76, 78, 78, 76, 78, 78, 77, 78, 78, 77, 78, 77, 78, 78, 77, 78, 78,\n",
      "         76, 78, 78, 76, 72, 73, 74, 75, 78, 82, 82, 80, 78, 80, 82, 83, 82, 82,\n",
      "         80, 75, 76, 81, 81, 79, 78, 79, 81, 83, 81, 78, 79, 81, 78, 79, 81, 78,\n",
      "         81, 83, 80, 80, 75, 75, 78, 78, 75, 80, 80, 78, 76, 74, 76, 78, 78, 78,\n",
      "         79, 78, 79, 78, 78, 75, 78, 78, 75, 80, 80, 80, 75, 75, 78, 78, 75, 80,\n",
      "         80, 78, 76, 74, 76, 78, 78, 78, 66, 71, 78, 73, 71, 66, 71, 78, 73, 71,\n",
      "         66, 71, 78, 78, 78, 76, 74, 73, 74, 73, 71, 66, 67, 69, 66, 67, 69, 66,\n",
      "         69, 75, 76, 75, 64, 66, 68, 70, 71, 73, 75, 76, 73, 71, 73, 73, 75, 76,\n",
      "         78, 76, 75, 73, 71, 75, 76, 75, 76, 73, 71, 73, 73, 75, 78, 80, 80, 78,\n",
      "         75, 71, 71, 73, 74, 76, 78, 78, 79, 78, 78, 78, 78, 79, 81, 79, 78, 76,\n",
      "         74, 76, 78, 78, 79, 78, 78, 78, 78, 71, 74, 71, 74, 76, 78, 79, 72, 73,\n",
      "         74, 75, 75, 76, 77, 85, 82, 80, 77, 80, 80, 77, 82, 82, 80, 78, 76, 78,\n",
      "         80, 80, 80, 80, 81, 80, 81, 80, 80, 77, 80, 80, 77, 82, 82, 82, 77, 80,\n",
      "         80, 77, 82, 82, 80, 78, 76, 78, 80, 80, 80, 80, 80, 85, 92, 80, 85, 68,\n",
      "         73, 80, 75, 73, 68, 73, 80, 80, 80, 78, 76, 75, 76, 75, 73, 68, 69, 71,\n",
      "         68, 69, 71, 68, 71, 84, 84, 82, 80, 82, 84, 85, 84, 84, 82, 77, 78, 83,\n",
      "         83, 81, 80, 81, 83, 85, 83, 80, 81, 83, 80, 81, 83, 80, 81,  2]])\n",
      "tensor([[ 1, 64, 64, 64, 64, 61, 64, 68, 66, 71, 71, 71, 71, 68, 66, 68, 66, 64,\n",
      "         64, 64, 64, 64, 61, 64, 68, 66, 63, 63, 64, 64, 66, 64, 71, 71, 69, 68,\n",
      "         69, 68, 71, 71, 69, 68, 69, 68, 66, 71, 71, 69, 68, 66, 66, 66, 66, 64,\n",
      "         66, 68, 68, 71, 71, 69, 68, 69, 68, 71, 71, 69, 68, 69, 66, 71, 71, 69,\n",
      "         68, 66, 66, 66, 66, 66, 68, 69, 68, 71, 71, 69, 68, 69, 68, 71, 71, 69,\n",
      "         68, 69, 68, 66, 71, 71, 69, 68, 66, 66, 66, 66, 66, 68, 69, 66, 68, 61,\n",
      "         63, 64, 64, 64, 66, 68, 66, 63, 64, 66, 64, 71, 71, 69, 68, 69, 68, 71,\n",
      "         71, 69, 68, 69, 68, 66, 71, 71, 69, 68, 66, 66, 66, 66, 66, 68, 69, 66,\n",
      "         68, 61, 63, 64, 64, 64, 66, 68, 66, 63, 64, 66,  2],\n",
      "        [ 1, 88, 81, 83, 84, 77, 79, 80, 72, 76, 80, 83, 80, 80, 73, 75, 76, 69,\n",
      "         71, 73, 78, 81, 85, 88, 85, 85, 85, 83, 81, 83, 88, 86, 85, 83, 91, 82,\n",
      "         90, 86, 81, 81, 82, 84, 85, 90, 87, 83, 88, 90, 88, 87, 84, 89, 86, 88,\n",
      "         81, 83, 84, 77, 79, 80, 72, 76, 80, 83, 80, 80, 73, 75, 76, 69, 71, 73,\n",
      "         78, 81, 85, 88, 85, 73, 78, 80, 85, 88, 85, 73, 78, 81, 85, 88, 85, 88,\n",
      "         85, 81, 83, 81, 77, 80, 78, 88, 88, 85, 81, 83, 80, 78,  2,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/1000:   1%|▏             | 22/1545 [00:01<00:51, 29.40it/s, loss=23.227, bar=5.481, pos=4.635, token=4.945, dur=5.700, phrase=2.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 81, 81, 81, 81, 81, 81, 80, 79, 79, 79, 79, 81, 83, 81, 81, 78, 83,\n",
      "         83, 83, 83, 81, 83, 80, 76, 76, 78, 81, 78, 83, 78, 81, 78, 85, 83, 81,\n",
      "         83, 85, 81, 78, 78, 81, 78, 79, 80, 82, 82, 82, 82, 82, 82, 85, 87, 82,\n",
      "         85, 87, 87, 83, 88, 87, 85, 82, 82, 82, 82, 82, 85, 87, 82, 85, 87, 87,\n",
      "         83, 88, 87, 85, 81, 81, 81, 81, 81, 81, 80, 79, 79, 79, 79, 81, 83, 81,\n",
      "         81, 78, 78, 83, 83, 83, 83, 81, 83, 80, 76, 76, 78, 81, 78, 83, 78, 81,\n",
      "         78, 85, 83, 81, 83, 85, 81, 78, 78, 81, 78, 79,  2],\n",
      "        [ 1, 78, 78, 78, 78, 78, 78, 78, 78, 73, 75, 76, 78, 80, 87, 85, 80, 78,\n",
      "         76, 78, 80, 76, 73, 76, 76, 75, 76, 83, 83, 80, 68, 73, 75, 76, 78, 80,\n",
      "         87, 85, 79, 78, 76, 78, 80, 76, 73, 68, 73, 76, 78, 79, 78, 76, 73, 81,\n",
      "         83, 85, 86, 85, 83, 81, 83, 85, 76, 73, 76, 76, 81, 83, 85, 86, 85, 83,\n",
      "         81, 88, 88, 85, 85, 82, 84, 85, 87, 88, 87, 87, 85, 87, 75, 75, 75, 72,\n",
      "         75, 75, 78, 75, 79, 75, 80, 82, 78, 80, 75, 85, 78, 78, 78, 78, 78, 78,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[ 1, 68, 69, 71, 64, 75, 73, 71, 64, 75, 73, 71, 64, 64, 66, 68, 68, 64,\n",
      "         71, 69, 68, 64, 71, 69, 68, 64, 64, 61, 66, 68, 69, 71, 64, 75, 73, 71,\n",
      "         64, 75, 73, 71, 64, 64, 69, 68, 68, 64, 71, 69, 68, 64, 71, 69, 68, 64,\n",
      "         64, 61, 64, 64, 74, 73, 73, 71, 74, 73, 73, 71, 74, 76, 64, 74, 73, 73,\n",
      "         71, 73, 64, 76, 75, 75, 73, 76, 75, 75, 73, 76, 78, 66, 75, 73, 73, 71,\n",
      "         71, 71, 73, 71, 71, 69, 69, 68, 69, 71, 64, 75, 75, 73, 71, 64, 75, 75,\n",
      "         73, 71, 64, 64, 69, 68, 68, 64, 69, 71, 69, 68, 64, 69, 71, 69, 68, 64,\n",
      "         64, 66, 68, 64,  2],\n",
      "        [ 1, 64, 64, 64, 64, 67, 66, 64, 71, 68, 64, 69, 69, 69, 71, 69, 64, 64,\n",
      "         64, 64, 64, 66, 64, 71, 68, 64, 61, 69, 61, 64, 68, 68, 64, 73, 71, 69,\n",
      "         68, 71, 69, 68, 66, 69, 68, 66, 64,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0]])\n",
      "tensor([[ 1, 68, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 73, 73, 73, 71,\n",
      "         71, 64, 68, 68, 68, 69, 69, 69, 69, 69, 68, 66, 68, 69, 69, 69, 69, 69,\n",
      "         69, 69, 71, 68, 69, 64, 76, 76, 78, 80, 80, 78, 76, 73, 76, 75, 76, 78,\n",
      "         76, 75, 73, 71, 76, 68, 71, 68, 71, 68, 69, 68, 66, 73, 69, 69, 69, 66,\n",
      "         63, 63, 64,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 63, 66, 68, 63, 64, 66, 75, 75, 76, 68, 71, 73, 75, 73, 71, 78, 75,\n",
      "         73, 71, 70, 66, 69, 68, 64, 75, 75, 73, 66, 66, 73, 71, 66, 66, 73, 71,\n",
      "         70, 68, 63, 66, 68, 63, 64, 66, 75, 75, 76, 68, 71, 73, 75, 73, 71, 78,\n",
      "         75, 73, 71, 70, 66, 69, 68, 64, 75, 75, 73, 66, 66, 73, 71, 66, 66, 73,\n",
      "         71, 70, 71, 72, 73, 66, 68, 69, 71, 72, 73, 66, 68, 69, 71, 67, 68, 71,\n",
      "         73, 76, 75, 71, 68, 74, 75, 68, 70, 71, 73, 74, 75, 68, 70, 71, 73, 71,\n",
      "         70, 66, 66, 66, 71, 68, 73, 63, 66, 68, 63, 64, 66, 75, 75, 76, 68, 71,\n",
      "         73, 75, 73, 71, 78, 75, 73, 71, 70, 66, 69, 68, 64, 75, 75, 73, 66, 66,\n",
      "         75, 75, 75, 76, 73, 71,  2]])\n",
      "tensor([[ 1, 59, 68, 66, 64, 63, 64, 61, 63, 59, 68, 69, 71, 71, 71, 68, 66, 68,\n",
      "         69, 66, 68, 69, 71, 73, 73, 69, 66, 66, 73, 71, 69, 71, 68, 66, 68, 69,\n",
      "         69, 69, 68, 69, 68, 66, 64, 59, 59, 68, 66, 64, 63, 64, 61, 63, 59, 68,\n",
      "         69, 71, 71, 71, 68, 66, 68, 69, 73, 71, 69, 71, 73, 73, 69, 66, 66, 73,\n",
      "         71, 71, 69, 71, 68, 66, 68, 69, 69, 69, 68, 69, 68, 66, 64, 59, 59, 64,\n",
      "         66, 68, 68, 68, 69, 68, 66, 64, 73, 73, 73, 74, 73, 71, 69, 68, 71, 71,\n",
      "         68, 66, 68, 69, 73, 71, 69, 71, 73, 71, 73, 75, 73, 75, 76, 76, 76, 71,\n",
      "         73, 71, 69, 68, 71, 69, 68, 66, 64, 63,  2],\n",
      "        [ 1, 71, 70, 69, 66, 71, 70, 69, 66, 71, 70, 69, 71, 73, 75, 76, 68, 71,\n",
      "         68, 66, 64, 64, 68, 71, 69, 68, 66, 66, 73, 75, 76, 75, 69, 69, 71, 73,\n",
      "         75, 73, 68, 68, 68, 69, 70, 71, 71, 70, 71, 68, 68, 66, 64, 64, 66, 68,\n",
      "         68, 70, 70, 68, 66, 70, 70, 68, 66, 73, 72, 73, 75, 71, 68, 71, 68, 66,\n",
      "         64, 64, 68, 71, 69, 68, 66, 66, 73, 75, 76, 75, 69, 69, 71, 73, 75, 73,\n",
      "         68, 68, 68, 69, 70, 73, 71, 70, 73, 71, 70, 71, 75, 73, 72, 73, 75, 73,\n",
      "         71, 70, 69, 66, 71, 70, 69, 66, 71, 70, 69, 71, 73, 75, 76, 68, 71, 68,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/1000:   2%|▎             | 30/1545 [00:01<00:45, 33.59it/s, loss=23.121, bar=5.527, pos=4.663, token=5.008, dur=5.609, phrase=2.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 61, 68, 66, 68, 64, 63, 61, 68, 68, 61, 64, 66, 68, 66, 68, 68, 61,\n",
      "         64, 64, 66, 68, 68, 68, 61, 64, 63, 61, 59, 61, 59, 56, 59, 61, 64, 64,\n",
      "         63, 61, 63, 61, 59, 56, 68, 68, 61, 64, 66, 68, 66, 68, 68, 68, 61, 64,\n",
      "         66, 68, 68, 68, 61, 64, 63, 63, 63, 61, 59, 61, 59, 56, 59, 61, 61, 61,\n",
      "         68, 66, 68, 64, 63, 61, 68, 68, 61, 64, 66, 68, 66, 68, 68, 61, 64, 64,\n",
      "         66, 68, 68, 68, 61, 64, 63, 63, 63, 61, 59, 61, 59, 56, 59, 61, 64, 64,\n",
      "         63, 61, 63, 61, 59, 56, 68, 68, 61, 64, 66, 68, 66, 68, 68, 68, 61, 64,\n",
      "         66, 68, 68, 61, 64, 63, 63, 63, 61, 59, 61, 59, 56, 59, 61, 68, 68, 68,\n",
      "         61, 64, 66, 68, 68, 68, 61, 64, 63, 63, 63, 61, 59, 61, 59, 56, 59, 61,\n",
      "          2],\n",
      "        [ 1, 54, 54, 54, 57, 57, 57, 59, 54, 54, 54, 54, 57, 57, 57, 59, 54, 57,\n",
      "         57, 57, 61, 59, 57, 59, 54, 54, 54, 57, 57, 59, 54, 54, 54, 57, 57, 59,\n",
      "         54, 54, 54, 54, 57, 57, 57, 59, 54, 64, 59, 64, 59, 66, 63, 63, 64, 59,\n",
      "         64, 59, 66, 63, 63, 64, 61, 61, 63, 59, 59, 64, 61, 61, 61, 63, 59, 59,\n",
      "         61, 66, 61, 66, 61, 54, 54, 54, 57, 57, 57, 59, 54, 57, 52, 57, 52, 59,\n",
      "         56, 56, 54, 54, 54, 57, 57, 57, 59, 54, 57, 52, 57, 52, 59, 56, 56, 57,\n",
      "         57, 57, 61, 59, 57, 59, 54, 57, 54, 54, 56, 52, 52, 54, 54, 57, 57, 59,\n",
      "         54, 57, 54, 54, 54, 56, 52, 52, 54, 54, 57, 57, 59, 54, 54, 59, 54,  2,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0]])\n",
      "tensor([[ 1, 75, 71, 78, 76, 75, 73, 71, 73, 78, 73, 66, 78, 76, 75, 73, 75, 71,\n",
      "         78, 81, 75, 83, 75, 81, 75, 83, 71, 80, 80, 80, 80, 66, 66, 66, 70, 70,\n",
      "         73, 73, 76, 76, 78, 66, 78, 76, 76, 75, 75, 75, 75, 75, 73, 73, 66, 73,\n",
      "         83, 81, 80, 81, 80, 80, 80, 80, 66, 66, 70, 73, 73, 73, 76, 76, 76, 76,\n",
      "         78, 66, 78, 76, 76, 75, 75, 75, 71, 78, 73,  2,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 76, 85, 83, 81, 80, 81, 85, 83, 81, 80, 81, 85, 85, 81, 86, 85, 85,\n",
      "         85, 86, 83, 76, 85, 83, 81, 80, 81, 85, 83, 81, 83, 81, 85, 85, 85, 81,\n",
      "         86, 76, 85, 85, 85, 86, 83, 85, 86, 88, 88, 88, 88, 88, 86, 86, 85, 86,\n",
      "         85, 78, 78, 86, 86, 85, 83, 81, 80, 81, 85, 83, 85, 83, 81, 80, 81, 76,\n",
      "         85, 83, 81, 80, 81, 81, 83, 83, 88, 85, 81, 83, 81, 81, 83, 85, 83, 81,\n",
      "         80, 81, 83, 76, 85, 83, 81, 80, 81, 84, 84, 84, 86, 83, 79, 76, 77, 84,\n",
      "         86, 84, 84, 84, 84, 86, 83, 79, 88, 88, 85, 88, 89, 87, 85, 83, 82, 83,\n",
      "         87, 85, 83, 85, 83, 87, 85, 87, 83, 88, 87, 87, 87, 88, 85, 78, 87, 85,\n",
      "         83, 82, 83, 78, 87, 85, 83, 87, 83, 87, 87, 87, 83, 88, 83, 87, 87, 87,\n",
      "         88, 85, 80, 89, 87, 85, 84, 85, 89, 87, 85, 87, 85, 89, 85, 89, 85, 90,\n",
      "         89, 89, 89, 90, 87, 80, 89, 87, 85, 84, 85, 80, 89, 87, 85, 89, 85, 89,\n",
      "         89, 89, 85, 90, 85, 89, 89, 89, 90, 87, 89, 87,  2]])\n",
      "tensor([[ 1, 90, 85, 87, 88, 90, 87, 85, 87, 83, 87, 85, 81, 85, 87, 88, 83, 78,\n",
      "         80, 92, 87, 85, 87, 85, 81, 78, 92, 87, 85, 87, 85, 81, 78, 90, 85, 81,\n",
      "         85, 83, 80, 78, 80, 75, 78, 76, 73, 76, 73, 71, 83, 78, 80, 81, 83, 80,\n",
      "         78, 80, 76, 80, 78, 74, 78, 80, 81, 76, 80, 81, 83, 85, 80, 81, 85, 88,\n",
      "         84, 85, 87, 88, 88, 92, 90, 84, 85, 87, 88, 88, 92, 88, 73, 75, 76, 76,\n",
      "         80, 76, 73, 71, 83, 90, 85, 87, 88, 90, 87, 85, 87, 83, 87, 85, 81, 85,\n",
      "         87, 88, 83, 78, 80, 92, 87, 85, 87, 85, 81, 78, 92, 87, 85, 87, 85, 81,\n",
      "         78, 90, 85, 81, 85, 83, 80, 76, 90, 85, 81, 85, 83, 85, 81, 80, 80, 81,\n",
      "         81, 80, 80, 81, 80, 80, 81, 85, 81, 80, 78, 76, 81, 80, 80, 81, 81, 80,\n",
      "         80, 81, 80, 80, 81, 85, 81, 80,  2],\n",
      "        [ 1, 68, 73, 80, 80, 78, 80, 78, 76, 78, 76, 75, 76, 73, 73, 78, 85, 85,\n",
      "         83, 85, 83, 81, 83, 81, 80, 81, 78, 80, 76, 78, 73, 76, 78, 80, 76, 78,\n",
      "         80, 83, 83, 80, 76, 73, 80, 76, 78, 80, 83, 85, 85, 83, 80, 78, 76, 78,\n",
      "         68, 73, 80, 80, 78, 80, 78, 76, 78, 76, 75, 76, 73, 73, 78, 85, 85, 83,\n",
      "         85, 83, 81, 83, 81, 80, 81, 78, 80, 76, 78, 73, 76, 78, 80, 80, 76, 78,\n",
      "         80, 83, 85, 83, 80, 78, 76, 73, 76, 73, 76, 73, 76, 73, 76,  2,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[ 1, 83, 83, 82, 82, 82, 81, 85, 80, 78, 76, 76, 73, 75, 71, 83, 83, 83,\n",
      "         82, 82, 82, 81, 85, 81, 80, 80, 78, 78, 80, 82, 87, 87, 78, 83, 71, 80,\n",
      "         80, 80, 80, 80, 71, 73, 76, 75, 73, 73, 71, 81, 81, 81, 81, 81, 71, 72,\n",
      "         73, 76, 75, 73, 73, 71, 83, 83, 83, 84, 84, 84, 84, 85, 83, 80, 77, 77,\n",
      "         80, 80, 80, 85, 80, 78, 71, 80, 80, 80, 80, 80, 71, 73, 76, 75, 73, 75,\n",
      "         71, 81, 81, 81, 81, 81, 71, 73, 76, 75, 73, 73, 71, 83, 83, 83, 84, 84,\n",
      "         84, 84, 85, 83, 80, 77, 77, 80, 78, 75, 78, 76, 76, 74, 76, 80, 83, 83,\n",
      "         76, 74, 76, 80, 83, 83, 83, 81, 83, 81, 81, 83, 81, 83, 81, 81, 80, 78,\n",
      "         80, 82, 85, 85, 80, 78, 80, 82, 85, 85, 80, 85, 85, 85, 80, 81, 83, 81,\n",
      "         80, 78, 71, 80, 80, 80, 80, 80, 71, 73, 76, 75, 73, 73, 71, 81, 81, 81,\n",
      "         81, 81, 71, 73, 76, 75, 73, 73, 71, 83, 83, 83, 84, 84, 84, 84, 85, 83,\n",
      "         80, 77, 77, 80, 78, 76, 78,  2],\n",
      "        [ 1, 66, 78, 66, 67, 78, 76, 75, 66, 65, 76, 74, 73, 71, 68, 71, 75, 74,\n",
      "         75, 76, 73, 66, 78, 66, 67, 78, 76, 75, 66, 65, 76, 74, 73, 71, 68, 71,\n",
      "         75, 75, 71, 66, 68, 70, 71, 72, 72, 72, 69, 69, 72, 72, 72, 69, 69, 72,\n",
      "         69, 72, 76, 76, 72, 68, 69, 71, 72, 73, 73, 73, 70, 70, 73, 73, 73, 70,\n",
      "         70, 73, 70, 73, 77, 77, 73, 74, 75, 78, 66, 67, 78, 76, 75, 66, 65, 76,\n",
      "         74, 73, 71, 68, 71, 75,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/1000:   2%|▎             | 38/1545 [00:01<00:43, 34.43it/s, loss=23.164, bar=5.596, pos=4.758, token=5.105, dur=5.519, phrase=2.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 80, 78, 80, 80, 80, 78, 80, 80, 78, 80, 80, 78, 80, 80, 80, 78, 80,\n",
      "         80, 78, 80, 73, 80, 78, 76, 78, 80, 78, 76, 78, 80, 76, 75, 73, 73, 80,\n",
      "         78, 76, 78, 80, 78, 76, 78, 80, 76, 75, 73, 76, 76, 76, 76, 76, 76, 76,\n",
      "         76, 76, 75, 73, 73, 76, 76, 76, 75, 73, 76, 75, 73, 75, 76, 73, 75, 76,\n",
      "         73, 75, 76, 76, 76, 75, 76, 75, 73, 76, 75, 73, 76, 75, 73, 71, 73, 73,\n",
      "         73, 75, 76, 76, 76, 75, 73, 76, 75, 73, 75, 76, 75, 73, 75, 76, 75, 73,\n",
      "         75, 76, 75, 73, 75, 76, 75, 73, 75, 76, 75, 73, 76, 75, 73, 76, 75, 73,\n",
      "         76, 75, 73, 75, 76, 75, 73, 71, 73, 73, 75, 76, 75, 73, 76, 78, 76, 75,\n",
      "         80, 81, 80, 81, 80, 71, 71, 73, 73, 73, 73, 71, 73, 73, 73, 73, 71, 73,\n",
      "         73, 73, 73, 73, 73, 85, 85, 71, 71, 73, 73, 73, 73, 71, 73, 73, 73, 73,\n",
      "         71, 73, 73, 73, 73, 73, 73, 85, 85, 73, 80, 78, 76, 78, 80, 78, 76, 78,\n",
      "         80, 76, 75, 73, 71, 73, 73, 73, 73, 80, 78, 76, 78, 80, 78, 76, 78, 80,\n",
      "         76, 75, 73, 71, 73, 73, 73, 75, 76, 76, 76, 76, 76, 75, 73, 76, 73, 73,\n",
      "         75, 76, 75, 73, 75, 76, 75, 73, 73, 75, 76, 76, 76, 76, 76, 73, 75, 76,\n",
      "         76, 76, 76, 73, 73, 75, 76, 75, 73, 75, 76, 75, 73, 75, 76, 75, 73, 73,\n",
      "         73, 75, 76, 76, 76, 76, 75, 73, 76, 75, 73, 76, 76, 76, 76, 76, 76, 75,\n",
      "         73, 81, 76, 75, 76, 76, 76, 76, 76, 76, 73, 76, 76, 76, 75, 73, 76, 75,\n",
      "         73, 75, 76, 75, 73, 71, 73,  2],\n",
      "        [ 1, 63, 64, 64, 63, 64, 63, 64, 63, 64, 66, 63, 64, 64, 63, 63, 66, 66,\n",
      "         66, 66, 66, 68, 69, 63, 64, 64, 63, 64, 63, 64, 63, 64, 66, 63, 64, 64,\n",
      "         63, 63, 69, 69, 68, 68, 66, 66, 64, 66, 61, 61, 61, 61, 63, 64, 61, 61,\n",
      "         61, 63, 59, 66, 66, 68, 68, 68, 68, 68, 68, 68, 68, 68, 66, 68, 68, 69,\n",
      "         68, 68, 68, 68, 68, 68, 66, 68, 66, 73, 66, 73, 73, 71, 69, 66, 73, 66,\n",
      "         73, 73, 71, 73, 71, 69, 73, 74, 73, 74, 73, 73, 73, 73, 78, 78, 78, 69,\n",
      "         71, 69, 66, 69, 71, 78, 78, 78, 69, 71, 69, 71, 66, 73, 66, 73, 76, 73,\n",
      "         71, 69, 66, 76, 73, 71, 69, 66, 69, 71, 73, 76, 73, 71, 69, 66, 76, 73,\n",
      "         71, 69, 66, 76, 73, 71, 69, 66, 69, 71, 73, 76, 73, 71, 69, 66, 76, 73,\n",
      "         71, 69, 66, 76, 73, 71, 69, 66, 63, 64, 64, 63, 64, 63, 64, 63, 64, 66,\n",
      "         63, 64, 64, 63, 63, 66, 66, 66, 66, 66, 68, 69, 63, 64, 64, 63, 64, 63,\n",
      "         64, 63, 64, 66, 63, 64, 64, 63, 63, 69, 69, 68, 68, 66, 66,  2,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[ 1, 70, 71, 68, 73, 70, 71, 68, 73, 70, 71, 69, 66, 63, 59, 70, 71, 69,\n",
      "         66, 63, 59, 70, 71, 68, 73, 70, 71, 73, 71, 68, 73, 70, 71, 68, 73, 70,\n",
      "         71, 69, 66, 63, 59, 70, 71, 69, 66, 63, 59, 70, 71, 68, 73, 70, 71, 73,\n",
      "         76, 73, 76, 73, 69, 68, 66, 69, 68, 68, 69, 68, 71, 68, 66, 66, 68, 66,\n",
      "         71, 69, 68, 71, 73, 76, 73, 76, 73, 69, 68, 66, 69, 68, 68, 64, 68, 64,\n",
      "         66, 64, 63, 66, 63, 64, 71, 71, 71, 69, 68, 69, 71, 68, 71, 69, 68, 71,\n",
      "         69, 68, 61, 69, 69, 69, 68, 66, 68, 69, 66, 78, 76, 75, 73, 71, 69, 68,\n",
      "         76, 73, 71, 68, 76, 73, 76, 73, 78, 76, 75, 78, 76, 75, 78, 76, 73, 71,\n",
      "         76, 73, 71, 68, 76, 73, 76, 73, 78, 76, 75, 78, 76, 75, 78, 76, 73, 71,\n",
      "         71, 71, 71, 69, 68, 69, 71, 68, 73, 73, 73, 71, 69, 71, 73, 66, 68, 69,\n",
      "         68, 69, 68, 69, 68, 68, 64, 68, 64, 68, 64, 68, 66, 66, 63, 66, 63, 66,\n",
      "          2],\n",
      "        [ 1, 76, 73, 73, 78, 78, 71, 71, 80, 80, 73, 73, 81, 81, 78, 78, 83, 80,\n",
      "         83, 80, 80, 76, 80, 76, 76, 73, 76, 73, 71, 73, 76, 80, 75, 75, 82, 82,\n",
      "         75, 80, 80, 77, 80, 77, 82, 82, 75, 75, 83, 80, 83, 80, 83, 80, 85, 85,\n",
      "         85, 83, 83, 83, 85, 83, 80, 76, 71, 83, 83, 85, 83, 81, 78, 73, 73, 78,\n",
      "         81, 85, 87, 85, 85, 81, 78, 76, 83, 83, 83, 85, 83, 80, 76, 71, 83, 83,\n",
      "         85, 83, 81, 78, 73, 73, 78, 78, 85, 87, 85, 85, 81, 78, 76, 83, 76, 78,\n",
      "         76, 78, 78, 78, 80, 80, 82, 80, 82, 82, 82, 83, 82, 83, 82, 83, 83, 87,\n",
      "         83, 85, 78, 78, 83, 83, 83, 85, 83, 80, 76, 71, 83, 85, 83, 81, 78, 73,\n",
      "         73, 78, 81, 85, 87, 85, 85, 81, 78,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0]])\n",
      "tensor([[ 1, 59, 59, 59, 61, 64, 64, 66, 68, 68, 66, 64, 68, 66, 64, 64, 63, 64,\n",
      "         61, 59, 61, 69, 68, 69, 61, 60, 59, 68, 67, 68, 59, 61, 59, 58, 66, 64,\n",
      "         66, 68, 66, 59, 59, 61, 64, 64, 66, 68, 68, 66, 64, 68, 66, 64, 64, 63,\n",
      "         64, 66, 63, 63, 64, 63, 61, 68, 67, 68, 69, 68, 61, 66, 65, 66, 68, 61,\n",
      "         64, 63, 61, 63, 64,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [ 1, 71, 73, 71, 68, 71, 76, 76, 71, 73, 71, 68, 71, 75, 71, 73, 71, 69,\n",
      "         71, 81, 81, 78, 71, 81, 81, 78, 80, 76, 71, 73, 71, 68, 71, 76, 76, 71,\n",
      "         73, 71, 68, 71, 75, 71, 73, 71, 69, 71, 81, 81, 78, 71, 81, 81, 78, 80,\n",
      "         76, 76, 78, 80, 76, 78, 75, 75, 75, 80, 78, 80, 76, 78, 75, 78, 80, 82,\n",
      "         78, 80, 77, 77, 77, 77, 80, 78, 76, 73, 71, 71, 73, 71, 68, 71, 76, 76,\n",
      "         71, 73, 71, 68, 71, 75, 71, 73, 81, 80, 78, 83, 73, 73, 73, 73, 81, 78,\n",
      "          2]])\n",
      "tensor([[ 1, 61, 64, 66, 64, 66, 69, 68, 69, 68, 64, 61, 61, 64, 66, 64, 66, 69,\n",
      "         68, 66, 66, 64, 73, 64, 73, 71, 69, 71, 64, 71, 71, 66, 69, 64, 66, 73,\n",
      "         64, 66, 73, 64, 66, 61, 64, 66, 64, 66, 69, 68, 69, 68, 64, 61, 61, 64,\n",
      "         66, 64, 66, 69, 68, 66, 66, 64, 73, 64, 73, 71, 69, 71, 64, 71, 71, 66,\n",
      "         69, 64, 66, 73, 64, 66, 73, 69, 66, 69, 66, 66, 76, 73, 69, 71, 64, 76,\n",
      "         76, 69, 66, 64, 66, 76, 64, 66, 76, 71, 73, 69, 66, 66, 76, 73, 69, 71,\n",
      "         64, 76, 76, 64, 66, 64, 66, 73, 64, 66, 73, 64, 66,  2],\n",
      "        [ 1, 73, 85, 85, 85, 85, 84, 85, 83, 80, 85, 85, 85, 85, 84, 85, 83, 80,\n",
      "         85, 85, 85, 85, 84, 85, 81, 80, 78, 83, 83, 83, 82, 83, 81, 78, 83, 83,\n",
      "         83, 82, 83, 81, 78, 83, 83, 83, 82, 83, 81, 78, 80, 83, 80, 76, 75, 73,\n",
      "         80, 80, 80, 80, 83, 83, 80, 76, 80, 80, 73, 80, 80, 80, 80, 83, 83, 80,\n",
      "         76, 78, 78, 78, 78, 78, 78, 81, 81, 78, 75, 78, 78, 78, 85, 83, 81, 80,\n",
      "         80, 79, 80, 76, 75, 73, 73, 80, 80, 79, 80, 83,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/1000:   2%|▎             | 38/1545 [00:01<01:04, 23.41it/s, loss=23.164, bar=5.596, pos=4.758, token=5.105, dur=5.519, phrase=2.186]\n",
      "  0%|                                                  | 0/1000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_l2m\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 96\u001b[0m, in \u001b[0;36mtrain_l2m\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_epoch) \u001b[38;5;28;01mas\u001b[39;00m _tqdm:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_epoch):\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m         train_running_loss, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# train_writer.add_scalars(\"train_epoch_loss\", {\"running\": train_running_loss, 'word': train_word_loss}, epoch)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# validation  \u001b[39;00m\n\u001b[1;32m    100\u001b[0m         valid_running_loss, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m valid(val_loader, model, epoch, total_epoch)\n",
      "Cell \u001b[0;32mIn[21], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, scheduler, epoch, total_epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 3) total loss\u001b[39;00m\n\u001b[1;32m     51\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m bar_loss \u001b[38;5;241m+\u001b[39m pos_loss \u001b[38;5;241m+\u001b[39m token_loss \u001b[38;5;241m+\u001b[39m dur_loss \u001b[38;5;241m+\u001b[39m phrase_loss\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/xailyr/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_l2m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1880d-891b-4e10-b289-272a3233c55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xailyr)",
   "language": "python",
   "name": "xailyr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
